{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH_N1jmCGpoY"
      },
      "source": [
        "# Artificial Neural Networks and Deep Learning - Homework 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMMZh83sNuD-"
      },
      "source": [
        "<h3>Useful links</h3>\n",
        "\n",
        "- [ImageDataGenerator](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator)\n",
        "- [keras_tuner](https://www.tensorflow.org/tutorials/keras/keras_tuner)\n",
        "- [bayesian-optimization-in-cnn](https://www.kaggle.com/code/toniesteves/bayesian-optimization-in-cnn)\n",
        "- [hyperparameter-search-with-bayesian-optimization-for-keras-cnn-classification-and-ensembling](https://machinelearningapplied.com/hyperparameter-search-with-bayesian-optimization-for-keras-cnn-classification-and-ensembling)\n",
        "-[NN-SVG](https://alexlenail.me/NN-SVG)\n",
        "- [tuning_playbook#contributing](https://github.com/google-research/tuning_playbook#contributing)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lj95TjCdGpoZ"
      },
      "source": [
        "| Overfitters del test set    | PoliMi ID |\n",
        "|:----------------------------|:---------:|\n",
        "|Pertino Paolo                | 10729600  |\n",
        "|Sandri Alberto               | 10698469  |\n",
        "|Simionato Enrico             | 10698193  |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmjLUGxqGpoa"
      },
      "source": [
        "In the first homework of the Artificial Neural Networks and Deep Learning course, a binary image classification task is proposed. The goal is to understand from images of plants whether they are healthy or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOsUppr8Gpoa"
      },
      "source": [
        "## Table of contents\n",
        "\n",
        "- [0. Preliminary operations](#0-preliminary-operations)\n",
        "    - [0.1 Connect to drive](#01-connect-to-drive)\n",
        "    - [0.2 Download and import libraries](#02-download-and-import-libraries)\n",
        "- [1. Dataset inspection](#1-dataset-inspection)\n",
        "    - [1.1 Loading the data](#11-loading-the-data)\n",
        "    - [1.2 Outliers removal](#12-outliers-removal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM35w8I3Gpoa"
      },
      "source": [
        "## 0. Preliminary operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3wS0zM2Gpoa"
      },
      "source": [
        "In the following section some preliminary operations in order to setup the environment correctly are performed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omSLbdLvhDRx"
      },
      "source": [
        "### 0.1 Connect to Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoaLQpvChLpb"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    print(\"Code running on Google Colab... Conncecting to Google Drive...\")\n",
        "    drive.mount('/gdrive')\n",
        "    %cd /gdrive/My Drive/[2023-2024] AN2DL/Homework 1\n",
        "except:\n",
        "    print(\"The code is not running on Google Colab...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHlAd-7IGpob"
      },
      "source": [
        "### 0.2 Download and import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7uloRUwGpoc"
      },
      "outputs": [],
      "source": [
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install keras\n",
        "!pip install tqdm\n",
        "!pip install scikit-learn\n",
        "!pip install tensorflow[and-cuda]\n",
        "!pip install shap\n",
        "!pip install pandas\n",
        "!pip install scipy\n",
        "!pip install Cython\n",
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_S1JfaW8bIN"
      },
      "outputs": [],
      "source": [
        "# Fix randomness and hide warnings\n",
        "seed = 42\n",
        "\n",
        "import os\n",
        "import warnings\n",
        "import logging\n",
        "import random\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n",
        "\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "random.seed(seed)\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "#import shap\n",
        "import cv2\n",
        "np.random.seed(seed)\n",
        "\n",
        "from tensorflow import keras as tfk\n",
        "from tensorflow.keras import layers as tfkl\n",
        "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "from IPython.display import Image\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "tf.autograph.set_verbosity(0)\n",
        "tf.get_logger().setLevel(logging.ERROR)\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "tf.random.set_seed(seed)\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "print(tf.__version__)\n",
        "print(f\"Num GPUs Available: {len(tf.config.list_physical_devices('GPU'))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO50lejkGpoc"
      },
      "source": [
        "## 1. Dataset inspection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BE2HxXt-Gpoc"
      },
      "source": [
        "In this section the content of the dataset is inspected and the images are prepared for later analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53DAEfQuI41_"
      },
      "source": [
        "### 1.1 Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Kf6RGYa6_wF"
      },
      "outputs": [],
      "source": [
        "data = np.load('data/public_data.npz', allow_pickle=True)\n",
        "\n",
        "images = data.get('data')\n",
        "labels = data.get('labels')\n",
        "\n",
        "images.shape, labels.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FZp3S8bGpoc"
      },
      "source": [
        "As we can see, the dataset is composed by `5200` colored images of size `96x96`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fElT3FZ7nBI"
      },
      "outputs": [],
      "source": [
        "images_normalized = (images / 255).astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkanBaGol1XG"
      },
      "outputs": [],
      "source": [
        "# Number of images to display\n",
        "num_img = 20\n",
        "\n",
        "# Create subplots for displaying items\n",
        "np.random.seed(312)\n",
        "fig, axes = plt.subplots(2, num_img // 2, figsize=(20, 9))\n",
        "for i in range(num_img):\n",
        "    ax = axes[i % 2, i % num_img // 2]\n",
        "    index_shown = np.random.randint(0, len(images))\n",
        "    ax.imshow(np.clip(images_normalized[index_shown], 0, 255))  # Display clipped item images\n",
        "    ax.axis('off')\n",
        "    ax.set_title(f\"{labels[index_shown]} - {index_shown}\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQjslbsyGpod"
      },
      "source": [
        "Hey! There are some outliers here! Images with index `4282` and `1903` are not plants :("
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeEEWbr_Gpod"
      },
      "source": [
        "### 1.2 Outliers removal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pz0zs6pNGpod"
      },
      "source": [
        "Let's remove the outliers we have just found and all their copies in the dataset (if any)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqLs1Nx08Wpo"
      },
      "outputs": [],
      "source": [
        "trololo = images[1903]\n",
        "shrek = images[4282]\n",
        "\n",
        "trolls = []\n",
        "for i in tqdm(range(len(images))):\n",
        "  if np.array_equiv(images[i], trololo) or np.array_equiv(images[i], shrek):\n",
        "    trolls.append(i)\n",
        "\n",
        "print(f\"In the dataset there are {len(trolls)} outliers.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYPpndVY9s3V"
      },
      "outputs": [],
      "source": [
        "# Deleting the trolls\n",
        "images_no_outliers = np.delete(images, trolls, axis = 0)\n",
        "labels_no_outliers = np.delete(labels, trolls, axis = 0)\n",
        "\n",
        "images_no_outliers.shape, labels_no_outliers.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2L_unKJuICkl"
      },
      "source": [
        "### 1.3 Storing and loading the dataset without outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItlCFflGIB9f"
      },
      "outputs": [],
      "source": [
        "# Storing the dataset without outliers\n",
        "np.save('data/public_data_no_outliers', images_no_outliers, allow_pickle=True)\n",
        "np.save('data/public_labels_no_outliers', labels_no_outliers, allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyWdX5qhIrgG"
      },
      "outputs": [],
      "source": [
        "images_no_outliers = np.load('data/public_data_no_outliers.npy', allow_pickle=True)\n",
        "labels_no_outliers = np.load('data/public_labels_no_outliers.npy', allow_pickle=True)\n",
        "\n",
        "images_no_outliers.shape, labels_no_outliers.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pfe2Rg9xKXlt"
      },
      "source": [
        "### 1.4 Plots and statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3q2a1ryPKeNb"
      },
      "outputs": [],
      "source": [
        "# Defining the counts\n",
        "counts = [sum(labels_no_outliers=='healthy'), sum(labels_no_outliers=='unhealthy')]\n",
        "\n",
        "# Define labels\n",
        "labels = ['healthy', 'unhealthy']\n",
        "\n",
        "# Define colors for the bars\n",
        "colors = ['lightgreen', 'salmon']\n",
        "\n",
        "# Create the barplot\n",
        "plt.bar(labels, counts, color=colors)\n",
        "\n",
        "# Add axes labels\n",
        "plt.xlabel('Classes - Health status')\n",
        "plt.ylabel('Counts')\n",
        "\n",
        "# Add title\n",
        "plt.title('Class distribution')\n",
        "\n",
        "# Add counts on top of the bars\n",
        "for i, value in enumerate(counts):\n",
        "  plt.text(labels[i], value, str(value), ha='center', va='bottom')\n",
        "\n",
        "# Plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHeF4G0oMI-1"
      },
      "outputs": [],
      "source": [
        "print(f\"\"\"\n",
        "  In the healthy class there are {counts[0]} samples which cover the {(counts[0] / (counts[0] + counts[1]))*100:.2f}% of the whole dataset.\n",
        "  In the unhealthy class there are {counts[1]} samples which cover the {(counts[1] / (counts[0] + counts[1]))*100:.2f}% of the whole dataset.\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZRKw0M3M4B2"
      },
      "source": [
        "The dataset is a little bit unbalanced towards the healthy class since there are 3101 samples in this class against 1903 of the other. However we think that this is not a big problem since the difference is not so big."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "en9fPhSQOV-j"
      },
      "source": [
        "### 1.5 Pre-processing (2 classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XX22AgQeI2Vk"
      },
      "outputs": [],
      "source": [
        "# Convert labels to one-hot encoding format\n",
        "y = np.zeros((labels_no_outliers.shape[0], 2))\n",
        "y[labels_no_outliers=='healthy', 0] = 1.\n",
        "y[labels_no_outliers=='unhealthy', 1] = 1.\n",
        "\n",
        "# Setting the input dataset to split\n",
        "X = (images_no_outliers / 255).astype(np.float32)\n",
        "\n",
        "# Defining the number of samples in the test set and in the validation set\n",
        "validation_size = 750\n",
        "test_size = 750\n",
        "\n",
        "# Split data into train_val and test sets\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, random_state=seed, test_size=test_size, stratify=np.argmax(y,axis=1))\n",
        "\n",
        "# Further split train_val into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=seed, test_size=validation_size, stratify=np.argmax(y_train_val,axis=1))\n",
        "\n",
        "# Print shapes of the datasets\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5.1 Pre-processing (1 class)"
      ],
      "metadata": {
        "id": "AwBMvcHXFCEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert labels to one-hot encoding format\n",
        "y = np.zeros((labels_no_outliers.shape[0],))\n",
        "y[labels_no_outliers=='healthy'] = 0.\n",
        "y[labels_no_outliers=='unhealthy'] = 1.\n",
        "\n",
        "# Setting the input dataset to split\n",
        "X = (images_no_outliers / 255).astype(np.float32)\n",
        "\n",
        "# Defining the number of samples in the test set and in the validation set\n",
        "validation_size = 750  # 15%\n",
        "test_size = 750  # 15%\n",
        "\n",
        "# Split data into train_val and test sets\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, random_state=seed, test_size=test_size, stratify=y)\n",
        "\n",
        "# Further split train_val into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=seed, test_size=validation_size, stratify=y_train_val)\n",
        "\n",
        "# Print shapes of the datasets\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
      ],
      "metadata": {
        "id": "ncoCimc6FCc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJC5WOwqMI-2"
      },
      "source": [
        "### 1.6 Mixup Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBCBlE79MI-2"
      },
      "outputs": [],
      "source": [
        "def mixup_images_labels(images : np.ndarray, labels : np.ndarray, alpha : float = 0.2, seed : int = 42):\n",
        "    \"\"\"\n",
        "    Mixup augmentation for a dataset of images and labels.\n",
        "\n",
        "    Parameters:\n",
        "        images (numpy.ndarray): An array of images with shape (num_images, height, width, channels).\n",
        "        labels (numpy.ndarray): An array of one-hot encoded labels with shape (num_images, num_classes).\n",
        "        alpha (float): Mixup hyperparameter, controls the interpolation ratio.\n",
        "\n",
        "    Returns:\n",
        "        mixed_images (numpy.ndarray): Mixed images.\n",
        "        mixed_labels (numpy.ndarray): Mixed labels.\n",
        "    \"\"\"\n",
        "    num_images = images.shape[0]\n",
        "\n",
        "    # Shuffle the dataset to ensure random mixing\n",
        "    np.random.seed(seed)\n",
        "    permutation = np.random.permutation(num_images)\n",
        "    shuffled_images = images[permutation]\n",
        "    shuffled_labels = labels[permutation]\n",
        "\n",
        "    lam = np.random.beta(alpha, alpha, num_images)\n",
        "    mixed_images = lam[:, np.newaxis, np.newaxis, np.newaxis] * images + (1 - lam[:, np.newaxis, np.newaxis, np.newaxis]) * shuffled_images\n",
        "    mixed_labels = lam[:, np.newaxis] * labels + (1 - lam[:, np.newaxis]) * shuffled_labels\n",
        "\n",
        "    return mixed_images, mixed_labels\n",
        "\n",
        "X_train_mixup, y_train_mixup = mixup_images_labels(X_train, y_train, 0.15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8R0iqOTmMI-3"
      },
      "outputs": [],
      "source": [
        "# Number of images to display\n",
        "num_img = 10\n",
        "\n",
        "# Create subplots for displaying items\n",
        "np.random.seed(312)\n",
        "fig, axes = plt.subplots(2, num_img // 2, figsize=(20, 9))\n",
        "for i in range(num_img):\n",
        "    ax = axes[i % 2, i % num_img // 2]\n",
        "    index_shown = np.random.randint(0, len(X_train_mixup))\n",
        "    ax.imshow(np.clip(X_train_mixup[index_shown], 0, 255))  # Display clipped item images\n",
        "    ax.axis('off')\n",
        "    ax.set_title(f\"{y_train_mixup[index_shown]} - {index_shown}\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine tuning, blocks activation"
      ],
      "metadata": {
        "id": "hd-AmLvjEWHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def activate_layers(model, name_layer, num_blocks_to_unfreeze, layers_to_skip=0, verbose=False):\n",
        "  \"\"\"\n",
        "  Makes trainable a requested number of blocks of the given model\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  activated_last_layers = False\n",
        "\n",
        "  model.get_layer(name_layer).trainable = False\n",
        "\n",
        "  layers = ft_efficient_model.get_layer(name_layer).layers\n",
        "  layers.reverse()\n",
        "\n",
        "  skipped_layers = 0\n",
        "\n",
        "  for i,layer in enumerate(layers):\n",
        "    if 'block' in layer.name:\n",
        "      if activated_last_layers:\n",
        "        index_of_character = layer.name.index('_')\n",
        "        current_block_name = layer.name[:index_of_character]\n",
        "\n",
        "        if current_block_name != previous_block_name and skipped_layers>=layers_to_skip:\n",
        "          num_blocks_to_unfreeze -= 1\n",
        "\n",
        "        elif current_block_name != previous_block_name and skipped_layers<layers_to_skip:\n",
        "          skipped_layers += 1\n",
        "          print(skipped_layers)\n",
        "\n",
        "        previous_block_name = current_block_name\n",
        "\n",
        "      else:\n",
        "        index_of_character = layer.name.index('_')\n",
        "        previous_block_name = layer.name[:index_of_character]\n",
        "        activated_last_layers = True\n",
        "\n",
        "      if num_blocks_to_unfreeze > 0 and skipped_layers>=layers_to_skip:\n",
        "        layer.trainable=True\n",
        "\n",
        "    else:\n",
        "      if activated_last_layers is False:\n",
        "        layer.trainable=True\n",
        "\n",
        "    if 'bn' in layer.name:\n",
        "      layer.trainable = False\n",
        "\n",
        "  if verbose:\n",
        "    for i, layer in enumerate(ft_efficient_model.get_layer(name_layer).layers):\n",
        "      print(i, layer.name, layer.trainable)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "uzSAFy9EEXQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exPX2uGUGpod"
      },
      "source": [
        "## 2. Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EEFEf9IMI-3"
      },
      "source": [
        "### 2.1 Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynoslnDKMI-3"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(preprocessed_input : np.ndarray, y_test : np.ndarray, threshold : int = 0.5, model_to_test : tf.keras.Model = None, model_name : str = None, one_class : bool = False) -> None:\n",
        "  '''\n",
        "    Plots the confusion matrix and displays classification metrics for the\n",
        "    specified model on the specified test data.\n",
        "\n",
        "    The function first checks whether a model or a model name is provided for\n",
        "    evaluation. If neither is provided, it will print an error message and\n",
        "    return without further execution.\n",
        "\n",
        "    If a model is provided (model_to_test is not None), the function uses it for\n",
        "    evaluation. If only a model name is provided, the function loads the saved\n",
        "    model with that name using tf.keras.models.load_model.\n",
        "\n",
        "    It then predicts labels for the test data using the model and calculates\n",
        "    the confusion matrix, as well as various classification metrics, including\n",
        "    accuracy, precision, recall, and F1-score.\n",
        "\n",
        "    The confusion matrix is plotted using a heatmap, and the classification\n",
        "    metrics are displayed in the console.\n",
        "\n",
        "    Parameters:\n",
        "      preprocessed_input (numpy.ndarray): Preprocessed input data for testing.\n",
        "      y_test (numpy.ndarray): Ground truth labels for the test data.\n",
        "      model_to_test (tf.keras.Model, optional): The model to evaluate. If\n",
        "      provided, this model will be used for evaluation.\n",
        "      model_name (str, optional): The name of a saved Keras model to load for\n",
        "      evaluation. If provided, this model will be loaded and used for\n",
        "      evaluation.\n",
        "      one_class (bool, optional): If True the output has one neuron. By default\n",
        "      output has two neurons.\n",
        "\n",
        "    Returns:\n",
        "      None\n",
        "  '''\n",
        "\n",
        "  if model_to_test is None and model_name is None:\n",
        "    print(\"You have to specify either a model name or pass directly the model you want to evaluate.\")\n",
        "    return\n",
        "\n",
        "  if model_to_test is not None:\n",
        "    # Loading the model\n",
        "    model = model_to_test\n",
        "  else:\n",
        "    model = tf.keras.models.load_model(model_name)\n",
        "\n",
        "  # Predict labels for the entire test set\n",
        "  predictions_logits = model.predict(preprocessed_input, verbose=0)\n",
        "\n",
        "  if one_class:\n",
        "    predicted_label = np.array([1. if prediction >= threshold else 0. for prediction in predictions_logits])\n",
        "    true_label = y_test\n",
        "  else:\n",
        "    predictions = np.array([[1., 0.] if sublist[0] >= threshold else [0., 1.] for sublist in predictions_logits])\n",
        "    true_label = np.argmax(y_test, axis=-1)\n",
        "    predicted_label = np.argmax(predictions, axis=-1)\n",
        "\n",
        "  # Compute the confusion\n",
        "  cm = confusion_matrix(true_label, predicted_label)\n",
        "\n",
        "  # Compute classification metrics\n",
        "  accuracy = accuracy_score(true_label, predicted_label)\n",
        "  precision = precision_score(true_label, predicted_label)\n",
        "  recall = recall_score(true_label, predicted_label)\n",
        "  f1 = f1_score(true_label, predicted_label)\n",
        "\n",
        "  # Display the computed metrics\n",
        "  print('Accuracy:', accuracy.round(4))\n",
        "  print('Precision:', precision.round(4))\n",
        "  print('Recall:', recall.round(4))\n",
        "  print('F1:', f1.round(4))\n",
        "\n",
        "  # Plot the confusion matrix\n",
        "  plt.figure(figsize=(10, 8))\n",
        "  sns.heatmap(cm.T, xticklabels=list(('healthy','unhealthy')), yticklabels=list(('healthy','unhealthy')), cmap='Blues', annot=True)\n",
        "  plt.xlabel('True labels')\n",
        "  plt.ylabel('Predicted labels')\n",
        "  plt.show()\n",
        "\n",
        "  if model_name:\n",
        "    del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaLcvEEtMI-3"
      },
      "outputs": [],
      "source": [
        "def plot_roc_curve(model : tf.keras.Model, X : np.ndarray, y : np.ndarray, plot_diagrams : bool = True) -> float:\n",
        "  '''\n",
        "    Plots Receiver Operating Characteristic (ROC) curves for different threshold\n",
        "    values and identifies the best threshold based on the highest AUC score.\n",
        "\n",
        "    This function calculates and plots ROC curves for a given model's predictions\n",
        "    on the input data, considering various threshold values (from 0.05 to 1 with\n",
        "    step 0.05). It also identifies the best threshold based on the highest Area\n",
        "    Under the Curve (AUC) score.\n",
        "\n",
        "    Parameters:\n",
        "        model (tf.keras.Model): The machine learning model for which the ROC\n",
        "                                curves are plotted.\n",
        "        X (numpy.ndarray): Input data used for model predictions.\n",
        "        y (numpy.ndarray): Ground truth labels for the input data.\n",
        "\n",
        "    Returns:\n",
        "        float: the best threshold retrieved by the aalysis of the AUC score.\n",
        "\n",
        "  '''\n",
        "\n",
        "  roc_curves = []\n",
        "  predictions_logits = model.predict(X)\n",
        "\n",
        "  thresholds = np.arange(0.05, 1.05, 0.05)\n",
        "  best_auc = 0\n",
        "  best_threshold = 0\n",
        "\n",
        "  for threshold in thresholds:\n",
        "      # Convert predictions using the current threshold\n",
        "      predictions = np.array([[1., 0.] if sublist[0] >= threshold else [0., 1.] for sublist in predictions_logits])\n",
        "\n",
        "      # Calculate the ROC curve\n",
        "      fpr, tpr, _ = roc_curve(y.argmax(axis=-1), predictions.argmax(axis=-1))\n",
        "\n",
        "      roc_auc = auc(fpr, tpr)\n",
        "      # Append the ROC curve data to the list\n",
        "      roc_curves.append((fpr, tpr, threshold))\n",
        "\n",
        "      if roc_auc > best_auc:\n",
        "        best_auc = roc_auc\n",
        "        best_threshold = threshold\n",
        "\n",
        "  # Plot ROC curves\n",
        "  if plot_diagrams:\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    for fpr, tpr, threshold in roc_curves:\n",
        "        plt.plot(fpr, tpr, label=f'Threshold: {threshold}')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', lw=2)  # Plot the diagonal line for reference\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.0])\n",
        "    plt.xlabel('False Positive Rate (FPR)')\n",
        "    plt.ylabel('True Positive Rate (TPR)')\n",
        "    plt.title('ROC Curve')\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "    print(f\"The best threshold is {best_threshold} with AUC score of {best_auc}\")\n",
        "  return best_threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixVW1kt-MI-4"
      },
      "outputs": [],
      "source": [
        "def plot_training(history):\n",
        "  '''\n",
        "    Plots training and validation performance metrics, including loss and\n",
        "    accuracy, and highlights the epoch with the highest validation accuracy.\n",
        "\n",
        "    This function takes a history dictionary that contains training and\n",
        "    validation metrics recorded during model training and visualizes the\n",
        "    training and validation performance over epochs. It also highlights\n",
        "    the epoch with the highest validation accuracy.\n",
        "\n",
        "    Parameters:\n",
        "        history (dict): A dictionary containing training and validation metrics\n",
        "        (e.g., loss, accuracy) recorded during model training.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "  '''\n",
        "\n",
        "  # Find the epoch with the highest validation accuracy\n",
        "  best_epoch = np.argmax(history['val_accuracy'])\n",
        "\n",
        "  # Plot training and validation performance metrics\n",
        "  plt.figure(figsize=(20, 5))\n",
        "\n",
        "  # Plot training and validation loss\n",
        "  plt.plot(history['loss'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=3)\n",
        "  plt.plot(history['val_loss'], label='Validation', alpha=0.8, color='#4D61E2', linewidth=3)\n",
        "  plt.legend(loc='upper left')\n",
        "  plt.title('Binary Crossentropy')\n",
        "  plt.grid(alpha=0.3)\n",
        "\n",
        "  plt.figure(figsize=(20, 5))\n",
        "\n",
        "  # Plot training and validation accuracy, highlighting the best epoch\n",
        "  plt.plot(history['accuracy'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=3)\n",
        "  plt.plot(history['val_accuracy'], label='Validation', alpha=0.8, color='#4D61E2', linewidth=3)\n",
        "  plt.plot(best_epoch, history['val_accuracy'][best_epoch], marker='*', alpha=0.8, markersize=10, color='#4D61E2')\n",
        "  plt.legend(loc='upper left')\n",
        "  plt.title('Accuracy')\n",
        "  plt.grid(alpha=0.3)\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3ClrbH3MI-4"
      },
      "outputs": [],
      "source": [
        "def k_fold_cross_validation(\n",
        "    train_data : np.ndarray,\n",
        "    train_labels : np.ndarray,\n",
        "    model_builder,\n",
        "    model_builder_parameters : tuple = (),\n",
        "    early_stopping_patience : int = 0,\n",
        "    epochs : int = 200,\n",
        "    batch_size : int = 32,\n",
        "    model_callbacks : list = None,\n",
        "    num_folds : int = 5,\n",
        "    verbose = 0,\n",
        "    seed : int = 42\n",
        "  ) -> dict:\n",
        "  '''\n",
        "  Perform K-fold cross-validation for training and evaluating the specified\n",
        "  model class.\n",
        "\n",
        "  This function performs K-fold cross-validation by splitting the training\n",
        "  data into multiple folds, training a model on each fold, and evaluating its\n",
        "  performance on the validation data. It returns a dictionary with information\n",
        "  about the training process for each fold.\n",
        "\n",
        "  Parameters:\n",
        "      train_data (numpy.ndarray): The training data used for cross-validation.\n",
        "      train_labels (numpy.ndarray): The corresponding ground truth labels for the training data.\n",
        "      model_builder (function): A function that builds and returns the model class of interest. This function should take model parameters as input.\n",
        "      model_builder_parameters (tuple, optional): Additional parameters to be passed to the model_builder function.\n",
        "      early_stopping_patience (int, optional): The number of epochs to wait for improvement during early stopping. Set this number equal to the one you have chosen for the early stopping.\n",
        "      epochs (int, optional): The maximum number of epochs for training each fold.\n",
        "      model_callbacks (list, optional): List of Keras callbacks to be applied during training.\n",
        "      num_folds (int, optional): The number of cross-validation folds.\n",
        "      seed (int, optional): Random seed for controlling the randomness of cross-validation.\n",
        "\n",
        "  Returns:\n",
        "      dict: A dictionary containing the training histories, validation scores, best epochs for early stopping, and best prediction thresholds for each fold.\n",
        "\n",
        "  '''\n",
        "\n",
        "  # Initialize lists to store training histories, scores, and best epochs\n",
        "  result = {}\n",
        "  histories = []\n",
        "  scores = []\n",
        "  best_epochs = []\n",
        "  best_prediction_thresholds = []\n",
        "\n",
        "  # Create a KFold cross-validation object\n",
        "  kfold = KFold(n_splits=num_folds, shuffle=True, random_state=seed)\n",
        "\n",
        "  # Loop through each fold\n",
        "  for fold_idx, (train_idx, valid_idx) in enumerate(kfold.split(train_data, train_labels)):\n",
        "\n",
        "    print(f\"Starting training on fold num: {fold_idx+1}\")\n",
        "\n",
        "    # Build a new dropout model for each fold\n",
        "    k_model = model_builder(*model_builder_parameters)\n",
        "\n",
        "    # Train the model on the training data for this fold\n",
        "    history = k_model.fit(\n",
        "      x = X_train_val[train_idx],\n",
        "      y = y_train_val[train_idx],\n",
        "      validation_data=(X_train_val[valid_idx], y_train_val[valid_idx]),\n",
        "      batch_size = batch_size,\n",
        "      epochs = epochs,\n",
        "      callbacks = model_callbacks,\n",
        "      verbose = verbose\n",
        "    ).history\n",
        "\n",
        "    # Evaluate the model on the validation data for this fold\n",
        "    score = k_model.evaluate(X_train_val[valid_idx], y_train_val[valid_idx], verbose=0)\n",
        "    scores.append(score[1])\n",
        "\n",
        "    # Calculate the best epoch for early stopping\n",
        "    best_epoch = len(history['loss']) - early_stopping_patience\n",
        "    best_epochs.append(best_epoch)\n",
        "\n",
        "    # Store the training history for this fold\n",
        "    histories.append(history)\n",
        "\n",
        "    # Store the best prediction threshold\n",
        "    best_prediction_thresholds.append(\n",
        "        plot_roc_curve(\n",
        "            k_model,\n",
        "            X_train_val[valid_idx],\n",
        "            y_train_val[valid_idx],\n",
        "            plot_diagrams = False)\n",
        "    )\n",
        "\n",
        "  result.update({\n",
        "      'histories' : histories,\n",
        "      'scores' : scores,\n",
        "      'best_epochs' : best_epochs,\n",
        "      'best_thresholds' : best_prediction_thresholds\n",
        "  })\n",
        "\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def undersample(X : np.ndarray, y : np.ndarray, one_class : bool = False) -> tuple:\n",
        "  '''\n",
        "    Perform undersampling on the given data.\n",
        "\n",
        "    Parameters:\n",
        "      X (numpy.ndarray): Preprocessed input data for training.\n",
        "      y (numpy.ndarray): Ground truth labels for the train data.\n",
        "      one_class (bool, optional): If True the output has one neuron. By default\n",
        "      output has two neurons.\n",
        "\n",
        "    Returns:\n",
        "      tuple: A tuple containing the given X and y undersampled.\n",
        "  '''\n",
        "\n",
        "  reshaped_X = X.reshape(X.shape[0],-1)\n",
        "\n",
        "  if not one_class:\n",
        "    y = np.argmax(y, axis=-1)\n",
        "\n",
        "  undersample = RandomUnderSampler()\n",
        "  undersampled_X, undersampled_y  = undersample.fit_resample(reshaped_X , y)\n",
        "\n",
        "  if not one_class:\n",
        "    undersampled_y = np.array([[1., 0.] if sample == 0. else [0., 1.] for sample in undersampled_y])\n",
        "\n",
        "  return undersampled_X.reshape(-1,96,96,3), undersampled_y"
      ],
      "metadata": {
        "id": "3qN4Pvg0chLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def oversample(X : np.ndarray, y : np.ndarray, one_class : bool = False) -> tuple:\n",
        "  '''\n",
        "    Perform oversampling on the given data.\n",
        "\n",
        "    Parameters:\n",
        "      X (numpy.ndarray): Preprocessed input data for training.\n",
        "      y (numpy.ndarray): Ground truth labels for the train data.\n",
        "      one_class (bool, optional): If True the output has one neuron. By default\n",
        "      output has two neurons.\n",
        "\n",
        "    Returns:\n",
        "      tuple: A tuple containing the given X and y overrsampled.\n",
        "  '''\n",
        "\n",
        "  reshaped_X = X.reshape(X.shape[0],-1)\n",
        "\n",
        "  if not one_class:\n",
        "    y = np.argmax(y, axis=-1)\n",
        "\n",
        "  oversample = RandomOverSampler()\n",
        "  oversampled_X, oversampled_y  = oversample.fit_resample(reshaped_X , y)\n",
        "\n",
        "  if not one_class:\n",
        "    oversampled_y = np.array([[1., 0.] if sample == 0. else [0., 1.] for sample in oversampled_y])\n",
        "\n",
        "  return oversampled_X.reshape(-1,96,96,3), oversampled_y"
      ],
      "metadata": {
        "id": "Tuy-Dvg7Dk1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lja4vs4jXZU8"
      },
      "source": [
        "### 2.2 Custom Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aRhYSy_Xku4"
      },
      "outputs": [],
      "source": [
        "def build_custom_baseline(input_shape, output_shape, seed=seed):\n",
        "  tf.random.set_seed(seed) # Execution on GPUs loses the deterministic execution guaranteed by the seed\n",
        "\n",
        "  # Build the neural network layer by layer\n",
        "  input_layer = tfkl.Input(shape=input_shape, name='Input')\n",
        "\n",
        "  # Convolutional layers are linear layers\n",
        "  conv1 = tfkl.Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='swish', name='conv1')(input_layer)\n",
        "\n",
        "  #conv1_1 = tfkl.Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='swish', name='conv1_1')(conv1)\n",
        "\n",
        "  pool1 = tfkl.MaxPooling2D(pool_size=(2,2), name='mp1')(conv1)\n",
        "\n",
        "  conv2 = tfkl.Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='swish', name='conv2')(pool1)\n",
        "\n",
        "  #conv2_1 = tfkl.Conv2D(filters=32,kernel_size=(3,3), padding='same',activation='swish', name='conv2_1')(conv2)\n",
        "\n",
        "  pool2 = tfkl.MaxPooling2D(pool_size =(2,2), name='mp2')(conv2)\n",
        "\n",
        "  conv3 = tfkl.Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='swish', name='conv3')(pool2)\n",
        "\n",
        "  #conv3_1 = tfkl.Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='swish', name='conv3_1')(conv3)\n",
        "\n",
        "  #pool3 = tfkl.MaxPooling2D(pool_size=(2,2), name='mp3')(conv3)\n",
        "\n",
        "  #conv4 = tfkl.Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='swish', name='conv4')(pool3)\n",
        "\n",
        "  #conv4_1 = tfkl.Conv2D(filters=128, kernel_size=(3,3),padding='same',activation='swish', name='conv4_1')(conv4)\n",
        "\n",
        "  gap = tfkl.GlobalAveragePooling2D(name='gap')(conv3)\n",
        "\n",
        "  classifier_layer1 = tfkl.Dense(\n",
        "      units=120,\n",
        "      activation='swish',\n",
        "      name='dense1'\n",
        "  )(gap)\n",
        "\n",
        "  classifier_layer2 = tfkl.Dense(\n",
        "      units=84,\n",
        "      activation='swish',\n",
        "      name='dense2'\n",
        "  )(classifier_layer1)\n",
        "\n",
        "  output_layer = tfkl.Dense(\n",
        "      units=output_shape,\n",
        "      activation='softmax',\n",
        "      name='Output'\n",
        "  )(classifier_layer2)\n",
        "\n",
        "  # Connect input and output through the Model class\n",
        "  model = tfk.Model(inputs=input_layer, outputs=output_layer, name='custom_baseline')\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.AdamW(), metrics=['accuracy'])\n",
        "\n",
        "  # Return the model\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ro8T5kpRZi3T"
      },
      "outputs": [],
      "source": [
        "input_shape = (96, 96, 3)\n",
        "output_shape = 2\n",
        "\n",
        "# Build the LeNet model and display its summary\n",
        "model = build_custom_baseline(input_shape, output_shape)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dh26gkDUajg_"
      },
      "outputs": [],
      "source": [
        "#dropout = tfkl.Dropout(dropout_rate, seed=seed)(hidden_activation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdvnsgPlarUh"
      },
      "outputs": [],
      "source": [
        "# Define the hyperparameters\n",
        "batch_size = 16\n",
        "epochs = 200\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = tfk.callbacks.EarlyStopping(monitor='val_accuracy', patience=20, mode='max', restore_best_weights=True)\n",
        "lr_patience = 5\n",
        "lr_scheduler = tfk.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_accuracy',     # Metric to monitor (validation mean squared error in this case)\n",
        "    patience=lr_patience,  # Number of epochs with no improvement after which learning rate will be reduced\n",
        "    factor=0.999,          # Factor by which the learning rate will be reduced (0.999 in this case)\n",
        "    mode='max',            # Mode to decide when to reduce learning rate ('min' means reduce when metric stops decreasing)\n",
        "    min_lr=1e-5            # Minimum learning rate\n",
        ")\n",
        "\n",
        "# Train the model and save its history\n",
        "history = model.fit(\n",
        "    x=X_train,\n",
        "    y=y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[early_stopping, lr_scheduler]\n",
        ").history\n",
        "\n",
        "# Save the trained model\n",
        "model.save('models/custom_baseline_multiple_conv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoDbrWVpbGdM"
      },
      "outputs": [],
      "source": [
        "# Find the epoch with the highest validation accuracy\n",
        "best_epoch = np.argmax(history['val_accuracy'])\n",
        "\n",
        "# Plot training and validation performance metrics\n",
        "plt.figure(figsize=(20, 5))\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.plot(history['loss'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=3)\n",
        "plt.plot(history['val_loss'], label='Validation', alpha=0.8, color='#4D61E2', linewidth=3)\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Binary Crossentropy')\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.figure(figsize=(20, 5))\n",
        "\n",
        "# Plot training and validation accuracy, highlighting the best epoch\n",
        "plt.plot(history['accuracy'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=3)\n",
        "plt.plot(history['val_accuracy'], label='Validation', alpha=0.8, color='#4D61E2', linewidth=3)\n",
        "plt.plot(best_epoch, history['val_accuracy'][best_epoch], marker='*', alpha=0.8, markersize=10, color='#4D61E2')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Accuracy')\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSlTfl3viYls"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix('models/custom_baseline_multiple_conv', X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tyiI1izkUyW"
      },
      "source": [
        "#### Training again on the whole dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Lx-EdELmEDA"
      },
      "outputs": [],
      "source": [
        "# Define the test size\n",
        "test_size = 100\n",
        "\n",
        "# Split data into train_val and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed, test_size=test_size, stratify=np.argmax(y,axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad5OvOAFmRrP"
      },
      "outputs": [],
      "source": [
        "input_shape = (96, 96, 3)\n",
        "output_shape = 2\n",
        "\n",
        "# Build the LeNet model and display its summary\n",
        "model = build_custom_baseline(input_shape, output_shape)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "od_wdVFtkYov"
      },
      "outputs": [],
      "source": [
        "# Define the hyperparameters\n",
        "batch_size = 16\n",
        "epochs = 200\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = tfk.callbacks.EarlyStopping(monitor='accuracy', patience=20, mode='max', restore_best_weights=True)\n",
        "\n",
        "# Define learning rate scheduler callback\n",
        "lr_patience = 5\n",
        "lr_scheduler = tfk.callbacks.ReduceLROnPlateau(monitor='accuracy', patience=lr_patience, factor=0.999, mode='max', min_lr=1e-5)\n",
        "\n",
        "# Train the model and save its history\n",
        "history = model.fit(\n",
        "    x=X,\n",
        "    y=y,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    callbacks=[early_stopping, lr_scheduler]\n",
        ").history\n",
        "\n",
        "# Save the trained model\n",
        "model.save('models/custom_baseline_multiple_conv_whole_dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Opa4BaPXn0J2"
      },
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "model.save('models/custom_baseline_multiple_conv_whole_dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWdG-Dv6lZtz"
      },
      "outputs": [],
      "source": [
        "# Find the epoch with the highest validation accuracy\n",
        "best_epoch = np.argmax(history['accuracy'])\n",
        "\n",
        "# Plot training and validation performance metrics\n",
        "plt.figure(figsize=(20, 5))\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.plot(history['loss'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=3)\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Binary Crossentropy')\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.figure(figsize=(20, 5))\n",
        "\n",
        "# Plot training and validation accuracy, highlighting the best epoch\n",
        "plt.plot(history['accuracy'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=3)\n",
        "plt.plot(best_epoch, history['accuracy'][best_epoch], marker='*', alpha=0.8, markersize=10, color='#4D61E2')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Accuracy')\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iZEIspll7nG"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix('models/custom_baseline_multiple_conv_whole_dataset', X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hasgF7uBNA4L"
      },
      "outputs": [],
      "source": [
        "# Find the epoch with the highest validation accuracy\n",
        "best_epoch = np.argmax(history['val_accuracy'])\n",
        "\n",
        "# Plot training and validation performance metrics\n",
        "plt.figure(figsize=(20, 5))\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.plot(history['loss'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=3)\n",
        "plt.plot(history['val_loss'], label='Validation', alpha=0.8, color='#4D61E2', linewidth=3)\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Binary Crossentropy')\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.figure(figsize=(20, 5))\n",
        "\n",
        "# Plot training and validation accuracy, highlighting the best epoch\n",
        "plt.plot(history['accuracy'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=3)\n",
        "plt.plot(history['val_accuracy'], label='Validation', alpha=0.8, color='#4D61E2', linewidth=3)\n",
        "plt.plot(best_epoch, history['val_accuracy'][best_epoch], marker='*', alpha=0.8, markersize=10, color='#4D61E2')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Accuracy')\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfTeR7dsSVPc"
      },
      "source": [
        "### 2.2 Paolo's models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VU3Ty3M7SVa-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfWlnSNCSVlH"
      },
      "source": [
        "### 2.3 Alberto's models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Custom model"
      ],
      "metadata": {
        "id": "a3bNxziaN1Mv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B11SULV4SVya"
      },
      "outputs": [],
      "source": [
        "def build_custom_baseline(input_shape, output_shape, seed=seed):\n",
        "  tf.random.set_seed(seed)\n",
        "  dropout_rate = 1/6\n",
        "\n",
        "  preprocessing = tfk.Sequential([\n",
        "        tfkl.RandomFlip('horizontal'),\n",
        "        tfkl.RandomFlip('vertical'),\n",
        "        tfkl.RandomRotation(0.2),\n",
        "        tfkl.RandomZoom(0.2),\n",
        "    ], name='Preprocessing')\n",
        "\n",
        "  # Build the neural network layer by layer\n",
        "  input_layer = tfkl.Input(shape=input_shape, name='Input')\n",
        "\n",
        "  preprocessing = preprocessing(input_layer)\n",
        "\n",
        "  # Convolutional layers are linear layers\n",
        "  x = tfkl.Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu', name='conv1')(preprocessing)\n",
        "  x = tfkl.Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu', name='conv1_1')(x)\n",
        "  x = tfkl.BatchNormalization(name='batchNorm1')(x)\n",
        "  x = tfkl.MaxPooling2D(pool_size=(2,2), name='mp1')(x)\n",
        "  x = tfkl.Dropout(dropout_rate, seed=seed)(x)\n",
        "\n",
        "  x = tfkl.Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu', name='conv2')(x)\n",
        "  x = tfkl.Conv2D(filters=64,kernel_size=(3,3), padding='same',activation='relu', name='conv2_1')(x)\n",
        "  x = tfkl.BatchNormalization(name='batchNorm2')(x)\n",
        "  x = tfkl.MaxPooling2D(pool_size =(2,2), name='mp2')(x)\n",
        "  x = tfkl.Dropout(dropout_rate, seed=seed)(x)\n",
        "\n",
        "  x = tfkl.Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu', name='conv3')(x)\n",
        "  x = tfkl.Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu', name='conv3_1')(x)\n",
        "  x = tfkl.BatchNormalization(name='batchNorm3')(x)\n",
        "  x = tfkl.MaxPooling2D(pool_size=(2,2), name='mp3')(x)\n",
        "  x = tfkl.Dropout(dropout_rate, seed=seed)(x)\n",
        "\n",
        "  x = tfkl.Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu', name='conv4')(x)\n",
        "  x = tfkl.Conv2D(filters=256, kernel_size=(3,3),padding='same',activation='relu', name='conv4_1')(x)\n",
        "  x = tfkl.BatchNormalization(name='batchNorm4')(x)\n",
        "  x = tfkl.GlobalAveragePooling2D(name='gap')(x)\n",
        "  x = tfkl.Dropout(dropout_rate, seed=seed)(x)\n",
        "\n",
        "  output_layer = tfkl.Dense(units=2, activation='softmax',name='Output')(x)\n",
        "\n",
        "  #classifier_layer2 = tfkl.Dense(units=84, activation='relu', name='dense2')(classifier_layer1)\n",
        "  #output_layer = tfkl.Dense(units=output_shape, activation='softmax', name='Output')(classifier_layer2)\n",
        "\n",
        "  # Connect input and output through the Model class\n",
        "  model = tfk.Model(inputs=input_layer, outputs=output_layer, name='custom_baseline')\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.AdamW(), metrics=['accuracy'])\n",
        "\n",
        "  # Return the model\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "image = tfk.utils.plot_model(model, show_shapes=True, expand_nested=True)\n",
        "\n",
        "# Define the output file path\n",
        "output_file = \"models/custom_baseline4/model_image.png\"\n",
        "\n",
        "# Save the image to the specified file\n",
        "with open(output_file, \"wb\") as f:\n",
        "    f.write(image.data)"
      ],
      "metadata": {
        "id": "7y2nQO8jOWgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRILaQKXM02l"
      },
      "outputs": [],
      "source": [
        "input_shape = (96, 96, 3)\n",
        "output_shape = 2\n",
        "\n",
        "# Build the LeNet model and display its summary\n",
        "model = build_custom_baseline(input_shape, output_shape)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OicZ3uP-M0uE"
      },
      "outputs": [],
      "source": [
        "# Define the hyperparameters\n",
        "batch_size = 16\n",
        "epochs = 200\n",
        "\n",
        "# Define callbacks\n",
        "early_stopping = tfk.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, mode='max', restore_best_weights=True)\n",
        "lr_scheduler = tfk.callbacks.ReduceLROnPlateau(monitor='val_accuracy', patience=5, factor=0.999, mode='max', min_lr=1e-5)\n",
        "\n",
        "# Train the model and save its history\n",
        "history = model.fit(\n",
        "    x=X_train,\n",
        "    y=y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[early_stopping, lr_scheduler]\n",
        ").history\n",
        "\n",
        "# Save the trained model\n",
        "model.save('models/custom_baseline2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-ruUGgTPCOO"
      },
      "outputs": [],
      "source": [
        "plot_training(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwFEGbJ1M0rD"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix('models/custom_baseline2', X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Transfer learning"
      ],
      "metadata": {
        "id": "7tWjpWAeOsxE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2 classes"
      ],
      "metadata": {
        "id": "fO4v-G1uSHVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.efficientnet import preprocess_input"
      ],
      "metadata": {
        "id": "_UPOpQL_PtQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "efficient = tf.keras.applications.EfficientNetB0(\n",
        "                    include_top=False,\n",
        "                    weights=\"imagenet\",\n",
        "                    input_shape=(96, 96, 3),\n",
        "                    pooling='avg',\n",
        "                  )\n",
        "#tfk.utils.plot_model(inceptionResnet, show_shapes=True)\n",
        "\n",
        "efficient.trainable = False\n",
        "\n",
        "# Define the augmentation\n",
        "preprocessing = tfk.Sequential([\n",
        "        tfkl.RandomFlip('horizontal_and_vertical'),\n",
        "        tfkl.RandomRotation(0.3),\n",
        "        tfkl.RandomContrast(0.1),\n",
        "        #tfkl.RandomBrightness(0.001)\n",
        "    ], name='Preprocessing')\n",
        "\n",
        "# Create an input layer with shape (96, 96, 3)\n",
        "inputs = tfk.Input(shape=(96, 96, 3))\n",
        "\n",
        "# Data augmentation\n",
        "augmentation = preprocessing(inputs)\n",
        "\n",
        "# Connect MobileNetV2 to the input\n",
        "x = efficient(augmentation)\n",
        "\n",
        "x = tfkl.Dense(256, kernel_initializer=tfk.initializers.HeUniform(), activation='relu')(x)\n",
        "x = tfkl.Dropout(1/12, seed=seed)(x)\n",
        "\n",
        "# Add a Dense layer with 2 units and softmax activation as the classifier\n",
        "outputs = tfkl.Dense(2, activation='softmax')(x)\n",
        "\n",
        "# Create a Model connecting input and output\n",
        "tl_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n",
        "\n",
        "# Compile the model with Categorical Cross-Entropy loss and Adam optimizer\n",
        "tl_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.AdamW(learning_rate=0.0005), metrics=['accuracy'])\n",
        "\n",
        "# Display model summary\n",
        "tl_model.summary()"
      ],
      "metadata": {
        "id": "sEvl4Z3RPNcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define callbacks\n",
        "early_stopping = tfk.callbacks.EarlyStopping(monitor='val_accuracy', patience=16, mode='max', restore_best_weights=True)\n",
        "lr_scheduler = tfk.callbacks.ReduceLROnPlateau(monitor='val_accuracy', patience=5, factor=0.999, mode='max', min_lr=1e-5)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "tl_history = tl_model.fit(\n",
        "    x = preprocess_input(X_train*255),\n",
        "    y = y_train,\n",
        "    batch_size = 16,\n",
        "    epochs = 100,\n",
        "    class_weight={0: 0.806869961444094, 1: 1.3146773272415762},\n",
        "    validation_data = (preprocess_input(X_val*255), y_val),\n",
        "    callbacks = [early_stopping, lr_scheduler]\n",
        ").history"
      ],
      "metadata": {
        "id": "zrVOuwvaPNZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tl_model.save('models/transfer_learning_efficientnetb0_0')"
      ],
      "metadata": {
        "id": "Cw4Cl6HpPNXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training(tl_history)"
      ],
      "metadata": {
        "id": "KzV-xrPtPNVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(preprocess_input(X_test*255), y_test, model_to_test=tl_model)"
      ],
      "metadata": {
        "id": "hblgyMHyYQmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del tl_model"
      ],
      "metadata": {
        "id": "osZ2Uhg1YVU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1 class"
      ],
      "metadata": {
        "id": "tmHqR__6SK3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.efficientnet import preprocess_input"
      ],
      "metadata": {
        "id": "hFF052zyZt-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "efficient = tf.keras.applications.EfficientNetB0(\n",
        "                    include_top=False,\n",
        "                    weights=\"imagenet\",\n",
        "                    input_shape=(96, 96, 3),\n",
        "                    pooling='avg',\n",
        "                  )\n",
        "\n",
        "efficient.trainable = False\n",
        "\n",
        "\n",
        "preprocessing = tfk.Sequential([\n",
        "        tfkl.RandomFlip('horizontal_and_vertical'),\n",
        "        #tfkl.RandomContrast(0.3),\n",
        "        #tfkl.RandomBrightness(0.0005),\n",
        "        #tfkl.ZeroPadding2D((2, 2)),\n",
        "        #tfkl.RandomCrop(96, 96),\n",
        "        tfkl.RandomRotation(0.5),\n",
        "    ], name='Preprocessing')\n",
        "\n",
        "classifier = tfk.Sequential([\n",
        "      #tfkl.Dense(64, activation='relu'),\n",
        "      #tfkl.Dropout(0.5),\n",
        "      #tfkl.Dense(32, activation='relu'),\n",
        "      #tfkl.Dropout(0.5),\n",
        "      tfkl.Dense(256, activation='relu', kernel_initializer=tfk.initializers.HeUniform()),#4096\n",
        "      tfkl.Dropout(0.5),\n",
        "      tfkl.Dense(128, activation='relu', kernel_initializer=tfk.initializers.HeUniform()),#1024\n",
        "      tfkl.Dropout(0.5),\n",
        "      tfkl.Dense(1, activation='sigmoid')\n",
        "], name='Classifier')\n",
        "\n",
        "# Create an input layer with shape (96, 96, 3)\n",
        "inputs = tfk.Input(shape=(96, 96, 3))\n",
        "\n",
        "# Data augmentation\n",
        "augmentation = preprocessing(inputs)\n",
        "\n",
        "# Connect MobileNetV2 to the input\n",
        "x = efficient(augmentation)\n",
        "\n",
        "outputs = classifier(x)\n",
        "\n",
        "tl_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n",
        "\n",
        "#tl_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "tl_model.compile(loss=tfk.losses.BinaryCrossentropy(), optimizer=tfk.optimizers.Adam(learning_rate=0.0001), metrics=['accuracy'])\n",
        "\n",
        "tl_model.summary()\n"
      ],
      "metadata": {
        "id": "R0WSPANkSNqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define callbacks\n",
        "early_stopping = tfk.callbacks.EarlyStopping(monitor='val_accuracy', patience=15, mode='max', restore_best_weights=True)\n",
        "lr_scheduler = tfk.callbacks.ReduceLROnPlateau(monitor='val_accuracy', patience=5, factor=0.999, mode='max', min_lr=1e-5)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "tl_history = tl_model.fit(\n",
        "    x = preprocess_input(X_train*255),\n",
        "    y = y_train,\n",
        "    batch_size = 16,\n",
        "    epochs = 100,\n",
        "    class_weight={0: 0.806869961444094, 1: 1.3146773272415762},\n",
        "    validation_data = (preprocess_input(X_val*255), y_val),\n",
        "    callbacks = [early_stopping, lr_scheduler]\n",
        ").history"
      ],
      "metadata": {
        "id": "3G-3K7Y-SWZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tl_model.save('models/transfer_learning_efficientnetb0_0_sig')"
      ],
      "metadata": {
        "id": "fo2NYT6rSWRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training(tl_history)"
      ],
      "metadata": {
        "id": "8CeFIbJnSWOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(preprocess_input(X_test*255), y_test, model_to_test=tl_model, one_class=True)"
      ],
      "metadata": {
        "id": "YOSKWRLBasYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del tl_model"
      ],
      "metadata": {
        "id": "H_MfVyTzYXhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fine tuning"
      ],
      "metadata": {
        "id": "d1pkUl5POslo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-load the model after transfer learning\n",
        "ft_model = tfk.models.load_model('models/transfer_learning_efficientnetb0_4_sig')\n",
        "ft_model.summary()"
      ],
      "metadata": {
        "id": "tigw-WpVO2NW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set all MobileNetV2 layers as trainable\n",
        "ft_model.get_layer('efficientnetb0').trainable = True\n",
        "for i, layer in enumerate(ft_model.get_layer('efficientnetb0').layers):\n",
        "   print(i, layer.name, layer.trainable)"
      ],
      "metadata": {
        "id": "twIGCgMvO2LK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze first N layers, e.g., until the 133rd one\n",
        "N = 222\n",
        "for i, layer in enumerate(ft_model.get_layer('efficientnetb0').layers[:N]):\n",
        "  layer.trainable=False\n",
        "#for i, layer in enumerate(ft_model.get_layer('efficientnetb0').layers[207:]):\n",
        "#  layer.trainable=False\n",
        "for i, layer in enumerate(ft_model.get_layer('efficientnetb0').layers[N:]):\n",
        "  if 'bn' in layer.name:\n",
        "    layer.trainable=False\n",
        "for i, layer in enumerate(ft_model.get_layer('efficientnetb0').layers):\n",
        "   print(i, layer.name, layer.trainable)\n",
        "ft_model.summary()"
      ],
      "metadata": {
        "id": "abxpm0C5O2Ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "ft_model.compile(loss=tfk.losses.BinaryCrossentropy(), optimizer=tfk.optimizers.AdamW(1e-5), metrics='accuracy')"
      ],
      "metadata": {
        "id": "DpjsvjV4O2Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = tfk.callbacks.EarlyStopping(monitor='val_accuracy', patience=15, mode='max', restore_best_weights=True)\n",
        "lr_scheduler = tfk.callbacks.ReduceLROnPlateau(monitor='val_accuracy', patience=3, factor=0.999, mode='max', min_lr=1e-7)\n",
        "\n",
        "# Fine-tune the model\n",
        "ft_history = ft_model.fit(\n",
        "    x = preprocess_input(new_X*255), # We need to apply the preprocessing thought for the MobileNetV2 network\n",
        "    y = new_y,\n",
        "    batch_size = 16,\n",
        "    epochs = 200,\n",
        "    validation_data = (preprocess_input(X_val*255), y_val), # We need to apply the preprocessing thought for the MobileNetV2 network\n",
        "    callbacks = [early_stopping, lr_scheduler]\n",
        ").history"
      ],
      "metadata": {
        "id": "JjNMAWqCO2C0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ft_model.save('models/ft_efficientnetb0_4_sig')"
      ],
      "metadata": {
        "id": "kv93VJP5O17A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del ft_model"
      ],
      "metadata": {
        "id": "LF49vOjmPA4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix_sigmoid('models/ft_efficientnetb0_4_sig', preprocess_input(X_test*255), y_test)"
      ],
      "metadata": {
        "id": "fNE6YiMRPFKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SLXNVuQUXak"
      },
      "source": [
        "### 2.4 Enrico's models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGJGMlvnUa1y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ensemble"
      ],
      "metadata": {
        "id": "PlRnp4WOPd_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "################################################################################\n",
        "########## IT IS ENOUGH TO RUN THIS CELL TO BE READY TO THE TRAINING ###########\n",
        "################################################################################\n",
        "################################################################################\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    print(\"Code running on Google Colab... Conncecting to Google Drive...\")\n",
        "    drive.mount('/gdrive')\n",
        "    %cd /gdrive/My Drive/[2023-2024] AN2DL/Homework 1\n",
        "except:\n",
        "    print(\"The code is not running on Google Colab...\")\n",
        "\n",
        "# Fix randomness and hide warnings\n",
        "seed = 42\n",
        "\n",
        "import os\n",
        "import warnings\n",
        "import logging\n",
        "import random\n",
        "import gc\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n",
        "\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "random.seed(seed)\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "#import shap\n",
        "import cv2\n",
        "np.random.seed(seed)\n",
        "\n",
        "from tensorflow import keras as tfk\n",
        "from tensorflow.keras import layers as tfkl\n",
        "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "from IPython.display import Image\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "tf.autograph.set_verbosity(0)\n",
        "tf.get_logger().setLevel(logging.ERROR)\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "tf.random.set_seed(seed)\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "print(tf.__version__)\n",
        "print(f\"Num GPUs Available: {len(tf.config.list_physical_devices('GPU'))}\")\n",
        "\n",
        "images_no_outliers = np.load('data/public_data_no_outliers.npy', allow_pickle=True)\n",
        "labels_no_outliers = np.load('data/public_labels_no_outliers.npy', allow_pickle=True)\n",
        "\n",
        "# Convert labels to one-hot encoding format\n",
        "y = np.zeros((labels_no_outliers.shape[0], 2))\n",
        "y[labels_no_outliers=='healthy', 0] = 1.\n",
        "y[labels_no_outliers=='unhealthy', 1] = 1.\n",
        "\n",
        "# Setting the input dataset to split\n",
        "X = (images_no_outliers / 255).astype(np.float32)\n",
        "\n",
        "# Defining the number of samples in the test set and in the validation set\n",
        "validation_size = 750\n",
        "test_size = 750\n",
        "\n",
        "# Split data into train_val and test sets\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, random_state=seed, test_size=test_size, stratify=np.argmax(y,axis=1))\n",
        "\n",
        "# Further split train_val into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=seed, test_size=validation_size, stratify=np.argmax(y_train_val,axis=1))\n",
        "\n",
        "# Print shapes of the datasets\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
        "\n",
        "del(images_no_outliers)\n",
        "del(labels_no_outliers)\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "rAjFzzzcZai-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ensemble class code"
      ],
      "metadata": {
        "id": "MYqHxsbKHquo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import clone_model\n",
        "from sklearn.utils import resample\n",
        "import gc\n",
        "import time\n",
        "\n",
        "\n",
        "class ModelEnsemble:\n",
        "  \"\"\"\n",
        "  Class that represent an ensemble model\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "  models: list(tensorflow.keras.Model)\n",
        "    list of the models of the ensemble\n",
        "  #histories: list(dict)\n",
        "    list of the histories of training, if the models are trained by the class\n",
        "    and not loaded\n",
        "  path_to_models: str\n",
        "    path where the models have to be stored or from where the models have to be\n",
        "    loaded\n",
        "  model_names_list: list(str)\n",
        "    list of the names of the models in the ensemble\n",
        "  num_models: int\n",
        "      number of models to insert in the ensemble\n",
        "  preprocessing_functions: list(funct)\n",
        "    list of the preprocessing functions to apply to the inputs of the the models\n",
        "  #X_train: numpy.ndarray\n",
        "    images used as train set if the models are trained by the class and not\n",
        "    loaded\n",
        "  #y_train: numpy.ndarray\n",
        "    labels of the images in the train set if the models are trained by the class\n",
        "    and not loaded\n",
        "  #X_val: numpy.ndarray\n",
        "    images used as validation set if the models are trained by the class and not\n",
        "    loaded\n",
        "  #y_val: numpy.ndarray\n",
        "    abels of the images in the train set if the models are trained by the class\n",
        "    and not loaded\n",
        "  #X_test: numpy.ndarray\n",
        "    images used as test set if the models are trained by the class and not\n",
        "    loaded\n",
        "  #y_test: numpy.ndarray\n",
        "    labels of the images in the test set if the models are trained by the class\n",
        "    and not loaded\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               model=None,\n",
        "               num_models=2,\n",
        "               preprocessing_input=None,\n",
        "               X=np.array([]),\n",
        "               y=np.array([]),\n",
        "               validation_size=1000,\n",
        "               test_size=1000,\n",
        "               batch_size=16,\n",
        "               epochs=10,\n",
        "               loss=tfk.losses.CategoricalCrossentropy(),\n",
        "               optimizer=tfk.optimizers.AdamW(),\n",
        "               metrics=['accuracy'],\n",
        "               callbacks=[tfk.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, mode='max', restore_best_weights=True)],\n",
        "               store_and_load=True,\n",
        "               evaluate=True,\n",
        "               ensemble_name='ensemble',\n",
        "               model_names_list=[],\n",
        "               preprocessing_functions=[],\n",
        "               path_to_models='models/',\n",
        "               seed=42,\n",
        "               verbose=True):\n",
        "    \"\"\"\n",
        "    Initializes the ensemble model\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: tensorflow.keras.Model\n",
        "      model to be used to create the models of the ensemble, the training always\n",
        "      starts from this model\n",
        "    num_models: int\n",
        "      number of models to insert in the ensemble\n",
        "    preprocessing_input: funct\n",
        "      preprocessing function to apply to the inputs of the neural network\n",
        "    X: numpy.ndarray\n",
        "      dataset of the images in input\n",
        "    y: numpy.ndarray\n",
        "      one-hot encoded labels associated to the images\n",
        "    validation_size: int | float\n",
        "      size in terms of samples or percentage of the validation set\n",
        "    test_size: int | float\n",
        "      size in terms of samples or percentage of the test set\n",
        "    batch_size: int\n",
        "      size of an input batch using in the training of the models\n",
        "    epochs: int\n",
        "      number of epochs of the training of the models\n",
        "    loss: tensorflow.keras.losses.Loss\n",
        "      loss to use during the training of the models\n",
        "    optimizer: tensorflow.keras.optimisers.Optimizer\n",
        "      optimizer to use during the training of the models\n",
        "    metrics: list(str)\n",
        "      list of the metrics to evaluate during the training\n",
        "    callbacks: list(tensorflow.keras.callbacks.Callback)\n",
        "      callbacks to run during the training of the models\n",
        "    store_and_load: bool\n",
        "      if True, the models of the ensemble are stored and deleted after training\n",
        "      in order to save space in the memory; they are all loaded again after the\n",
        "      training of last model has ended\n",
        "    evaluate: bool\n",
        "      if True, the ensemble is evaluated on the test set after the training, if\n",
        "      the models are trained by the class and not loaded\n",
        "    ensemble_name: str\n",
        "      name to give to the ensemble model, the models are stored using the\n",
        "      pattern: path_to_models + ensemble_name + index of the trained model\n",
        "    model_names_list: list(str)\n",
        "      list of the names of the models in the ensemble\n",
        "    preprocessing_functions: list(funct)\n",
        "      list of the preprocessing functions to apply to the inputs of the the\n",
        "      models\n",
        "    path_to_models: str\n",
        "      path where the models have to be stored or from where the models have to be\n",
        "      loaded\n",
        "    seed: int\n",
        "      seed to use to allow reproducibility\n",
        "    verbose: bool\n",
        "      if True, textual information about the operations performed by the object\n",
        "      are provided\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    self.models = []\n",
        "    #self.histories = []\n",
        "    self.path_to_models = path_to_models\n",
        "    self.model_names_list = []\n",
        "\n",
        "    if model is None:\n",
        "      # Storing the names of the models in the ensemble\n",
        "      self.model_names_list = model_names_list\n",
        "      self.num_models = len(model_names_list)\n",
        "      self.preprocessing_functions = preprocessing_functions\n",
        "    else:\n",
        "      self.num_models = num_models\n",
        "      self.preprocessing_functions = [preprocessing_input for i in range(num_models)]\n",
        "      self.train_ensemble(model, num_models, preprocessing_input, X, y,\n",
        "                          validation_size, test_size, batch_size, epochs, loss,\n",
        "                          optimizer, metrics, callbacks, store_and_load,\n",
        "                          evaluate, ensemble_name, path_to_models, seed, verbose)\n",
        "\n",
        "  def train_ensemble(self,\n",
        "                     model,\n",
        "                     num_models,\n",
        "                     preprocessing_input,\n",
        "                     X,\n",
        "                     y,\n",
        "                     validation_size,\n",
        "                     test_size,\n",
        "                     batch_size,\n",
        "                     epochs,\n",
        "                     loss,\n",
        "                     optimizer,\n",
        "                     metrics,\n",
        "                     callbacks,\n",
        "                     store_and_load,\n",
        "                     evaluate,\n",
        "                     ensemble_name,\n",
        "                     path_to_models,\n",
        "                     seed,\n",
        "                     verbose):\n",
        "\n",
        "    \"\"\"\n",
        "    Trains the ensemble model\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: tensorflow.keras.Model\n",
        "      model to be used to create the models of the ensemble, the training always\n",
        "      starts from this model\n",
        "    num_models: int\n",
        "      number of models to insert in the ensemble\n",
        "    preprocessing_input: funct\n",
        "      preprocessing function to apply to the inputs of the neural network\n",
        "    X: numpy.ndarray\n",
        "      dataset of the images in input\n",
        "    y: numpy.ndarray\n",
        "      one-hot encoded labels associated to the images\n",
        "    validation_size: int | float\n",
        "      size in terms of samples or percentage of the validation set\n",
        "    test_size: int | float\n",
        "      size in terms of samples or percentage of the test set\n",
        "    batch_size: int\n",
        "      size of an input batch using in the training of the models\n",
        "    epochs: int\n",
        "      number of epochs of the training of the models\n",
        "    loss: tensorflow.keras.losses.Loss\n",
        "      loss to use during the training of the models\n",
        "    optimizer: tensorflow.keras.optimisers.Optimizer\n",
        "      optimizer to use during the training of the models\n",
        "    metrics: list(str)\n",
        "      list of the metrics to evaluate during the training\n",
        "    callbacks: list(tensorflow.keras.callbacks.Callback)\n",
        "      callbacks to run during the training of the models\n",
        "    store_and_load: bool\n",
        "      if True, the models of the ensemble are stored and deleted after training\n",
        "      in order to save space in the memory; they are all loaded again after the\n",
        "      training of last model has ended\n",
        "    evaluate: bool\n",
        "      if True, the ensemble is evaluated on the test set after the training, if\n",
        "      the models are trained by the class and not loaded\n",
        "    ensemble_name: str\n",
        "      name to give to the ensemble model, the models are stored using the\n",
        "      pattern: path_to_models + ensemble_name + index of the trained model\n",
        "    path_to_models: str\n",
        "      path where the models have to be stored or from where the models have to be\n",
        "      loaded\n",
        "    seed: int\n",
        "      seed to use to allow reproducibility\n",
        "    verbose: bool\n",
        "      if True, textual information about the operations performed by the object\n",
        "      are provided\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Record the start time\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Split data into train_val and test sets\n",
        "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, random_state=seed, test_size=test_size, stratify=np.argmax(y,axis=1))\n",
        "\n",
        "    # Further split train_val into train and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=seed, test_size=validation_size, stratify=np.argmax(y_train_val,axis=1))\n",
        "\n",
        "    # Set the sets in the object\n",
        "    #self.X_test = X_test\n",
        "    #self.y_test = y_test\n",
        "    #self.X_val = X_val\n",
        "    #self.y_val = y_val\n",
        "    #self.X_train = X_train\n",
        "    #self.y_train = y_train\n",
        "\n",
        "    # Print shapes of the datasets\n",
        "    if verbose:\n",
        "      print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "      print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
        "      print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
        "      print()\n",
        "\n",
        "    # Training the models of the ensemble\n",
        "    for i in range(num_models):\n",
        "      # Cloning the model to train (the weights are randomly initialized)\n",
        "      new_model = clone_model(model)\n",
        "      # Copying the weights\n",
        "      new_model.set_weights(model.get_weights())\n",
        "\n",
        "      # Create a new instance of the optimizer using the configuration\n",
        "      new_optimizer = type(optimizer).from_config(optimizer.get_config())\n",
        "      #new_optimizer=tfk.optimizers.AdamW()\n",
        "\n",
        "      # Compiling the model\n",
        "      new_model.compile(loss=loss, optimizer=new_optimizer, metrics=metrics)\n",
        "\n",
        "      # Displaying the summary of the model\n",
        "      if verbose:\n",
        "        new_model.summary()\n",
        "\n",
        "      if verbose:\n",
        "        print(f'The learning of the model {i} started')\n",
        "        print()\n",
        "\n",
        "      # Bootstrapping the dataset\n",
        "      indexes = resample(range(X_train.shape[0]), random_state=seed, stratify=y_train)\n",
        "      new_X_train = X_train[indexes]\n",
        "      new_y_train = y_train[indexes]\n",
        "\n",
        "      # Running the garbage collector\n",
        "      gc.collect()\n",
        "\n",
        "      # Training a model\n",
        "      ft_history = new_model.fit(\n",
        "          x = self.preprocessing_functions[i](new_X_train*255),\n",
        "          y = new_y_train,\n",
        "          batch_size = batch_size,\n",
        "          epochs = epochs,\n",
        "          validation_data = (self.preprocessing_functions[i](X_val*255), y_val),\n",
        "          callbacks = callbacks\n",
        "      ).history\n",
        "\n",
        "      if store_and_load:\n",
        "        if verbose:\n",
        "          print(f'Storing model {ensemble_name}{i} started into {path_to_models}')\n",
        "          print()\n",
        "\n",
        "        # Storing the model, if requested, in order to save space\n",
        "        new_model.save(f'{path_to_models}{ensemble_name}{i}')\n",
        "        # Adding the name of the model to the list of the names of the models\n",
        "        self.model_names_list.append(f'{ensemble_name}{i}')\n",
        "\n",
        "        # Deleting the model\n",
        "        del new_model\n",
        "      else:\n",
        "        self.models.append(new_model)\n",
        "\n",
        "      #self.histories.append(ft_history)\n",
        "\n",
        "    # Loading the models if they were stored and deleted during the training\n",
        "    #if store_and_load:\n",
        "    #  for i in range(num_models):\n",
        "    #    self.models.append(tfk.models.load_model(f'{path_to_models}{ensemble_name}{i}'))\n",
        "\n",
        "    # Recording the end time\n",
        "    end_time = time.time()\n",
        "    # Calculating and printing the elapsed time\n",
        "    if verbose:\n",
        "      print(f\"Elapsed time: {(end_time - start_time)/3600} hours\")\n",
        "\n",
        "    # Evaluating the model\n",
        "    if evaluate:\n",
        "      self.evaluate()\n",
        "\n",
        "  def get_models(self):\n",
        "    \"\"\"\n",
        "    Returns the models that compose the ensemble\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list(tensorflow.keras.Model)\n",
        "      list of models that compose the ensemble\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    return self.models\n",
        "\n",
        "  def get_histories(self):\n",
        "    \"\"\"\n",
        "    Returns the histories of the train of the models composing the ensemble\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "      list of histories of the train of the models composing the ensemble\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    return self.histories\n",
        "\n",
        "\n",
        "  def predict_one_model(self, X, model_name, i, verbose=True):\n",
        "    \"\"\"\n",
        "    Predicts the probability distribution of the labels of the given inputs for\n",
        "    the i-th model of the ensemble\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X: numpy.ndarray\n",
        "      input images on which the prediction has to be computed\n",
        "    model_name: str\n",
        "      name of the model in to use to compute the predictions\n",
        "    i: int\n",
        "      index of the model in the ensemble to use to compute the predictions\n",
        "    verbose: bool\n",
        "      if True, textual information about the operations performed by the object\n",
        "      are provided\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list(numpy.ndarray)\n",
        "      list of the predicted probability distribution of the labels\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if verbose:\n",
        "        print(f'Loading the model {model_name} ...')\n",
        "    model = tfk.models.load_model(self.path_to_models+model_name)\n",
        "    y_pred = model.predict(self.preprocessing_functions[i](X))\n",
        "\n",
        "    # Deleting the model\n",
        "    del model\n",
        "    # Running the garbage collector\n",
        "    gc.collect()\n",
        "\n",
        "    return y_pred\n",
        "\n",
        "\n",
        "  def predict(self, X, mode='conf_weighted', weight_models=False, prob=False, verbose=True):\n",
        "    \"\"\"\n",
        "    Predicts the labels of the input images\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X: numpy.ndarray\n",
        "      input images on which the prediction has to be computed\n",
        "    mode: str\n",
        "      'conf_weighted', if the prediction has to be done considering the confidence\n",
        "      that the models have in predicting a class, 'majority', if the prediction\n",
        "      has to be done using majority voting\n",
        "    #weight_models: bool\n",
        "\n",
        "    prob: bool\n",
        "      if True, the probabilities of the positive class are returned instead of\n",
        "      the actual predicted class\n",
        "    verbose: bool\n",
        "      if True, textual information about the operations performed by the object\n",
        "      are provided\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    numpy.ndarray\n",
        "      predicted labels of the input images | probabilities of the positive class\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO we can try to change the way we compute the final labels and we weight\n",
        "    # the models\n",
        "\n",
        "    # Predicting the probability distribution\n",
        "    y_pred = np.array([])\n",
        "    if mode == 'conf_weighted':\n",
        "      y_pred = self.predict_one_model(X, self.model_names_list[0], 0)\n",
        "      for i,model_name in enumerate(self.model_names_list[1:]):\n",
        "        y_pred += self.predict_one_model(X, model_name, 0)\n",
        "        # Running the garbage collector\n",
        "        gc.collect()\n",
        "      y_pred = y_pred[:,1]/len(self.model_names_list) if prob else tf.argmax(y_pred, axis=-1).numpy()\n",
        "    elif mode == 'majority':\n",
        "      y_pred = tf.round(self.predict_one_model(X, self.model_names_list[0], 0)).numpy()\n",
        "      for i,model_name in enumerate(self.model_names_list[1:]):\n",
        "        y_pred += tf.round(self.predict_one_model(X, model_name, i)).numpy()\n",
        "        # Running the garbage collector\n",
        "        gc.collect()\n",
        "      y_pred = y_pred[:,1]/len(self.model_names_list) if prob else tf.argmax(y_pred, axis=-1).numpy()\n",
        "\n",
        "    return y_pred\n",
        "\n",
        "  def evaluate(self):\n",
        "    \"\"\"\n",
        "    Evaluates the ensemble model on the test set\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Computing the predictions\n",
        "    predictions = self.predict(self.X_test*255)\n",
        "\n",
        "    # Creating the confusion matrix\n",
        "    cm = confusion_matrix(np.argmax(self.y_test, axis=-1), predictions)\n",
        "\n",
        "    # Computing the classification metrics\n",
        "    accuracy = accuracy_score(np.argmax(self.y_test, axis=-1), predictions)\n",
        "    precision = precision_score(np.argmax(self.y_test, axis=-1), predictions)\n",
        "    recall = recall_score(np.argmax(self.y_test, axis=-1), predictions)\n",
        "    f1 = f1_score(np.argmax(self.y_test, axis=-1), predictions)\n",
        "\n",
        "    # Displaying the computed metrics\n",
        "    print('Accuracy:', accuracy.round(4))\n",
        "    print('Precision:', precision.round(4))\n",
        "    print('Recall:', recall.round(4))\n",
        "    print('F1:', f1.round(4))\n",
        "\n",
        "    # Plotting the confusion matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm.T, xticklabels=list(('healthy','unhealthy')), yticklabels=list(('healthy','unhealthy')), cmap='Blues', annot=True)\n",
        "    plt.xlabel('True labels')\n",
        "    plt.ylabel('Predicted labels')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "lGzfAk6UPi_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Models to use in the ensemble"
      ],
      "metadata": {
        "id": "BuL9eQJDHmoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### EfficientnetB0"
      ],
      "metadata": {
        "id": "Azot2Q2pJm55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.efficientnet import preprocess_input as preprocess_input_efficientnet\n",
        "\n",
        "efficientnet = tf.keras.applications.EfficientNetB0(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=(96, 96, 3),\n",
        "    pooling=\"avg\",\n",
        "\n",
        ")\n",
        "\n",
        "#tfk.utils.plot_model(efficientnet, show_shapes=True)"
      ],
      "metadata": {
        "id": "MlLRfdcxPi60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the supernet as feature extractor, i.e. freeze all its weigths\n",
        "efficientnet.trainable = False\n",
        "\n",
        "# Create an input layer with shape (96, 96, 3)\n",
        "inputs = tfk.Input(shape=(96, 96, 3))\n",
        "\n",
        "# Defining and evaluating the augmentation layer\n",
        "preprocessing = tfk.Sequential([\n",
        "      tfkl.RandomFlip('horizontal', name='RandomFlip_horizontal'),\n",
        "      tfkl.RandomFlip('vertical', name='RandomFlip_vertical'),\n",
        "      tfkl.RandomRotation(0.3, name='RandomRotation'),\n",
        "      tfkl.RandomContrast(0.3),\n",
        "  ], name='Preprocessing')\n",
        "\n",
        "\n",
        "preprocessed = preprocessing(inputs)\n",
        "\n",
        "# Connect to the input\n",
        "x = efficientnet(preprocessed)\n",
        "\n",
        "# Adding layers for the classification\n",
        "dropout_rate = 1/12\n",
        "\n",
        "dense = tfkl.Dense(128, activation='relu', kernel_initializer=tfk.initializers.HeUniform())(x)\n",
        "dropout = tfkl.Dropout(dropout_rate, seed=seed)(dense)\n",
        "\n",
        "# Add a Dense layer with 2 units and softmax activation as the classifier\n",
        "outputs = tfkl.Dense(2, activation='softmax')(dropout)\n",
        "\n",
        "# Create a Model connecting input and output\n",
        "tf_efficient_model = tfk.Model(inputs=inputs, outputs=outputs, name='elementar_mode_ensemble')\n",
        "\n",
        "# Compile the model with Categorical Cross-Entropy loss and Adam optimizer\n",
        "#tf_efficient_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.AdamW(), metrics=['accuracy'])\n",
        "\n",
        "# Display model summary\n",
        "tf_efficient_model.summary()"
      ],
      "metadata": {
        "id": "6tZIU9AXPi9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble = ModelEnsemble(model=tf_efficient_model,\n",
        "                         preprocessing_input=preprocess_input_efficientnet,\n",
        "                         num_models=50,\n",
        "                         X=X,\n",
        "                         y=y,\n",
        "                         validation_size=100,\n",
        "                         test_size=500,\n",
        "                         ensemble_name='ensemble_2_')"
      ],
      "metadata": {
        "id": "LmlxYei-PjCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Custom model"
      ],
      "metadata": {
        "id": "5CtiRpw1K9eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a convolutional block function with multiple parameters\n",
        "def conv_block_1(x, filters, kernel_size, padding='same', downsample=True, activation='relu', stack=2, name=''):\n",
        "\n",
        "  # If downsample is True, apply max-pooling\n",
        "  if downsample:\n",
        "    x = tfkl.MaxPooling2D(name='MaxPool_' + name)(x)\n",
        "\n",
        "  # Apply a stack of convolutional layers with specified filters, kernel size, and activation\n",
        "  for s in range(stack):\n",
        "    x = tfkl.Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, name='Conv_' + name + str(s+1))(x)\n",
        "    x = tfkl.Activation(activation, name='Activation_' + name + str(s+1))(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "# Define the model\n",
        "def build_custom_baseline_augmentation(input_shape=(96, 96, 3), output_shape=2, learning_rate=0.001, seed=42):\n",
        "\n",
        "  # Input layer\n",
        "  input_layer = tfkl.Input(shape=input_shape, name='Input_Layer')\n",
        "\n",
        "  # Define a preprocessing Sequential model with random flip, zero padding, and random crop\n",
        "  preprocessing = tfk.Sequential([\n",
        "        tfkl.RandomFlip('horizontal', name='RandomFlip_horizontal'),\n",
        "        tfkl.RandomFlip('vertical', name='RandomFlip_vertical'),\n",
        "        tfkl.RandomRotation(0.3, name='RandomRotation'),\n",
        "        #tfkl.RandomBrightness(0.001, name='RandomBrigtness'),\n",
        "        tfkl.RandomContrast(0.3),\n",
        "    ], name='Preprocessing')\n",
        "\n",
        "  # Apply preprocessing to input layer\n",
        "  x0 = preprocessing(input_layer)\n",
        "\n",
        "  # Create convolutional blocks\n",
        "  x1 = conv_block_1(x=x0, filters=32, kernel_size=3, downsample=False, stack=1, name='1')\n",
        "  x2 = conv_block_1(x=x1, filters=32, kernel_size=3, downsample=True, stack=1, name='2')\n",
        "  x3 = conv_block_1(x=x2, filters=64, kernel_size=3, downsample=True, stack=1, name='3')\n",
        "  x4 = conv_block_1(x=x3, filters=64, kernel_size=3, downsample=True, stack=1, name='4')\n",
        "  x5 = conv_block_1(x=x4, filters=128, kernel_size=3, downsample=True, stack=1, name='5')\n",
        "  dropout_rate = 1/6\n",
        "  dropout = tfkl.Dropout(dropout_rate, seed=seed)(x5)\n",
        "\n",
        "  # Global Average Pooling and classifier\n",
        "  x = tfkl.GlobalAveragePooling2D(name='GlobalAveragePooling')(dropout)\n",
        "  x = tfkl.Dense(output_shape, name='Dense')(x)\n",
        "  output_activation = tfkl.Activation('softmax', name='Softmax')(x)\n",
        "\n",
        "  # Create the model\n",
        "  model = tfk.Model(inputs=input_layer, outputs=output_activation, name='baseline_augmentation')\n",
        "\n",
        "  # Define optimizer, loss, and metrics\n",
        "  optimizer = tfk.optimizers.AdamW(learning_rate, weight_decay=5e-4)\n",
        "  loss = tfk.losses.CategoricalCrossentropy()\n",
        "  metrics = ['accuracy']\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
        "  return model\n",
        "\n",
        "input_shape = (96, 96, 3)\n",
        "output_shape = 2\n",
        "\n",
        "# Build the LeNet model and display its summary\n",
        "custom_model = build_custom_baseline_augmentation(input_shape, output_shape, learning_rate=0.001)\n",
        "custom_model.summary()"
      ],
      "metadata": {
        "id": "wvGT89e8LAHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_costom(X):\n",
        "  return X/255\n",
        "ensemble = ModelEnsemble(model=custom_model,\n",
        "                         preprocessing_input=preprocess_costom,\n",
        "                         num_models=50,\n",
        "                         X=X,\n",
        "                         y=y,\n",
        "                         validation_size=100,\n",
        "                         test_size=500,\n",
        "                         ensemble_name='ensemble_custom_model_01_')"
      ],
      "metadata": {
        "id": "5VGaJ3XDLALp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Custom model complex"
      ],
      "metadata": {
        "id": "prR573jTJt9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a residual convolutional block with optional batch normalization\n",
        "def conv_residual_block(x, filters, kernel_size, padding='same', downsample=True, activation='relu', stack=2, batch_norm=True, name=''):\n",
        "\n",
        "    # If downsample is True, apply max-pooling\n",
        "    if downsample:\n",
        "        x = tfkl.MaxPooling2D(name='MaxPool_' + name)(x)\n",
        "\n",
        "    # Create a copy of the input for the residual connection\n",
        "    x_ = x\n",
        "\n",
        "    # Apply a stack of convolutional layers to the copy\n",
        "    for s in range(stack):\n",
        "        x_ = tfkl.Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, name='Conv_' + name + str(s+1))(x_)\n",
        "        if batch_norm:\n",
        "            x_ = tfkl.BatchNormalization(name='BatchNorm_' + name + str(s+1))(x_)\n",
        "        x_ = tfkl.Activation(activation, name='Activation_' + name + str(s+1))(x_)\n",
        "\n",
        "    # If downsample is True, apply a 1x1 convolution to match the number of filters\n",
        "    if downsample:\n",
        "        x = tfkl.Conv2D(filters=filters, kernel_size=1, padding=padding, name='Conv_' + name + 'skip')(x)\n",
        "\n",
        "    # Add the original and the processed copy to create the residual connection\n",
        "    x = tfkl.Add(name='Add_' + name)([x_, x])\n",
        "\n",
        "    return x\n",
        "\n",
        "# Define a function to build a VGG18-like model with residual blocks\n",
        "def build_model(input_shape=(96, 96, 3), output_shape=2, learning_rate=1e-4, seed=seed):\n",
        "\n",
        "    # Input layer\n",
        "    input_layer = tfkl.Input(shape=input_shape, name='Input_Layer')\n",
        "\n",
        "    # Define a preprocessing Sequential model with random flip, zero padding, and random crop\n",
        "    preprocessing = tfk.Sequential([\n",
        "        tfkl.RandomFlip('horizontal', name='RandomFlip_horizontal'),\n",
        "        tfkl.ZeroPadding2D((2, 2), name='ZeroPadding_2x2'),\n",
        "        tfkl.RandomCrop(input_shape[0], input_shape[1], name='RandomCrop')\n",
        "    ], name='Preprocessing')\n",
        "\n",
        "    # Apply preprocessing to the input layer\n",
        "    x0 = preprocessing(input_layer)\n",
        "\n",
        "    # Initial convolution with batch normalization and activation\n",
        "    x0 = tfkl.Conv2D(filters=64, kernel_size=3, padding='same', name='Conv0')(x0)\n",
        "    x0 = tfkl.BatchNormalization(name='BatchNorm0')(x0)\n",
        "    x0 = tfkl.Activation('relu', name='ReLU0')(x0)\n",
        "\n",
        "    # Create residual blocks\n",
        "    x1 = conv_residual_block(x=x0, filters=64, kernel_size=3, downsample=False, stack=2, name='1')\n",
        "    x1 = conv_residual_block(x=x1, filters=64, kernel_size=3, downsample=False, stack=2, name='2')\n",
        "\n",
        "    x2 = conv_residual_block(x=x1, filters=128, kernel_size=3, downsample=True, stack=2, name='3')\n",
        "    x2 = conv_residual_block(x=x2, filters=128, kernel_size=3, downsample=False, stack=2, name='4')\n",
        "\n",
        "    x3 = conv_residual_block(x=x2, filters=256, kernel_size=3, downsample=True, stack=2, name='5')\n",
        "    x3 = conv_residual_block(x=x3, filters=256, kernel_size=3, downsample=False, stack=2, name='6')\n",
        "\n",
        "    x4 = conv_residual_block(x=x3, filters=512, kernel_size=3, downsample=True, stack=2, name='7')\n",
        "    x4 = conv_residual_block(x=x4, filters=512, kernel_size=3, downsample=False, stack=2, name='8')\n",
        "\n",
        "    # Global Average Pooling and classifier\n",
        "    x = tfkl.GlobalAveragePooling2D(name='GlobalAveragePooling')(x4)\n",
        "    x = tfkl.Dense(output_shape, name='Dense')(x)\n",
        "    output_activation = tfkl.Activation('softmax', name='Softmax')(x)\n",
        "\n",
        "    # Create the model\n",
        "    model = tfk.Model(inputs=input_layer, outputs=output_activation, name='VGG18_Residual')\n",
        "\n",
        "    # Define optimizer, loss, and metrics\n",
        "    # AdamW is an Adam optimizer which applies weight_decay to network layers,\n",
        "    # i.e it's another way to apply l2 regularization to the whole network\n",
        "    optimizer = tfk.optimizers.AdamW(learning_rate, weight_decay=5e-4)\n",
        "    loss = tfk.losses.CategoricalCrossentropy()\n",
        "    metrics = ['accuracy']\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
        "    return model\n",
        "\n",
        "custom_model = build_model()\n",
        "#residual_model.summary()\n",
        "#tfk.utils.plot_model(residual_model, expand_nested=True, show_shapes=True)"
      ],
      "metadata": {
        "id": "js1F5D5tIoSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_costom(X):\n",
        "  return X/255\n",
        "ensemble = ModelEnsemble(model=custom_model,\n",
        "                         preprocessing_input=preprocess_costom,\n",
        "                         num_models=50,\n",
        "                         X=X,\n",
        "                         y=y,\n",
        "                         validation_size=100,\n",
        "                         test_size=500,\n",
        "                         ensemble_name='ensemble_custom_model_01_')"
      ],
      "metadata": {
        "id": "hHkZolfgIJ2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing Ensemble through loading"
      ],
      "metadata": {
        "id": "8Y6QkLb77UD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "# Altro modo per caricare i modelli e fare l'ensemble, non serve runnare questa\n",
        "# cella\n",
        "################################################################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "\n",
        "\n",
        "\n",
        "# It is possible also to ensemble different models (it could be risky)\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input as preprocess_input_efficientnet\n",
        "\n",
        "def preprocess_costom(X):\n",
        "  return X/255\n",
        "\n",
        "path_to_models = 'models/'\n",
        "model_names = []\n",
        "num_models = 44\n",
        "for i in range(num_models):\n",
        "  model_names.append(f'ensemble_custom_model_01_{i}')\n",
        "\n",
        "preprocessing_functions = [preprocess_costom for i in range(num_models)]\n",
        "\n",
        "loaded_ensemble = ModelEnsemble(model_names_list=model_names,\n",
        "                                preprocessing_functions=preprocessing_functions,\n",
        "                                path_to_models=path_to_models)"
      ],
      "metadata": {
        "id": "iniRSm_7PjFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modes = ['conf_weighted', 'majority']\n",
        "seed = 42\n",
        "validation_size=100\n",
        "test_size=500\n",
        "\n",
        "# Split data into train_val and test sets\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, random_state=seed, test_size=test_size, stratify=np.argmax(y,axis=1))\n",
        "\n",
        "# Further split train_val into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=seed, test_size=validation_size, stratify=np.argmax(y_train_val,axis=1))\n",
        "\n",
        "del(X)\n",
        "del(y)\n",
        "\n",
        "# Running the garbage collector\n",
        "gc.collect()\n",
        "\n",
        "for mode in modes:\n",
        "\n",
        "  # Computing the predictions\n",
        "  predictions = loaded_ensemble.predict(X_test*255, mode=mode)\n",
        "\n",
        "  # Creating the confusion matrix\n",
        "  cm = confusion_matrix(np.argmax(y_test, axis=-1), predictions)\n",
        "\n",
        "  # Computing the classification metrics\n",
        "  accuracy = accuracy_score(np.argmax(y_test, axis=-1), predictions)\n",
        "  precision = precision_score(np.argmax(y_test, axis=-1), predictions)\n",
        "  recall = recall_score(np.argmax(y_test, axis=-1), predictions)\n",
        "  f1 = f1_score(np.argmax(y_test, axis=-1), predictions)\n",
        "\n",
        "  # Displaying the computed metrics\n",
        "  print('Accuracy:', accuracy.round(4))\n",
        "  print('Precision:', precision.round(4))\n",
        "  print('Recall:', recall.round(4))\n",
        "  print('F1:', f1.round(4))\n",
        "\n",
        "  # Plotting the confusion matrix\n",
        "  plt.figure(figsize=(10, 8))\n",
        "  sns.heatmap(cm.T, xticklabels=list(('healthy','unhealthy')), yticklabels=list(('healthy','unhealthy')), cmap='Blues', annot=True)\n",
        "  plt.xlabel('True labels')\n",
        "  plt.ylabel('Predicted labels')\n",
        "  plt.show()\n",
        "\n",
        "  # Running the garbage collector\n",
        "  gc.collect()"
      ],
      "metadata": {
        "id": "lItoho8C9IlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For the submissions"
      ],
      "metadata": {
        "id": "JXNbiD6koUFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE USED IN THE SUBMITS\n",
        "\n",
        "import os\n",
        "import numpy\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input as preprocess_input_efficientnet\n",
        "\n",
        "class model:\n",
        "    def __init__(self, path):\n",
        "        self.models = []\n",
        "        model_names_list = ['ensemble0', 'ensemble1', 'ensemble2', 'ensemble3', 'ensemble4', 'ensemble5', 'ensemble6', 'ensemble7', 'ensemble8']\n",
        "        for model_name in model_names_list:\n",
        "            self.models.append(tf.keras.models.load_model('models/'+model_name))#os.path.join(path, model_name)))\n",
        "\n",
        "    def predict(self, X):\n",
        "        out = self.predict2(preprocess_input_efficientnet(X))\n",
        "        return out\n",
        "\n",
        "    def predict_prob(self, X):\n",
        "        y_pred = []\n",
        "        for model in self.models:\n",
        "            y_pred.append(model.predict(X))\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "    def predict2(self, X, prob=False):\n",
        "        y_pred_prob = self.predict_prob(X)\n",
        "        y_pred_models = []\n",
        "        for pred in y_pred_prob:\n",
        "            y_pred_models.append(tf.argmax(pred, axis=-1))\n",
        "        y_pred = numpy.array(y_pred_models)\n",
        "\n",
        "        return tf.convert_to_tensor(numpy.mean(y_pred, axis=0)) if prob else tf.convert_to_tensor(numpy.round(numpy.mean(y_pred, axis=0)))"
      ],
      "metadata": {
        "id": "nKT3RZQikEW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = model('path')"
      ],
      "metadata": {
        "id": "P4l1f5V5kO9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing the predictions\n",
        "predictions = model1.predict(X_test*255)\n",
        "\n",
        "# Creating the confusion matrix\n",
        "cm = confusion_matrix(np.argmax(y_test, axis=-1), predictions)\n",
        "\n",
        "# Computing the classification metrics\n",
        "accuracy = accuracy_score(np.argmax(y_test, axis=-1), predictions)\n",
        "precision = precision_score(np.argmax(y_test, axis=-1), predictions)\n",
        "recall = recall_score(np.argmax(y_test, axis=-1), predictions)\n",
        "f1 = f1_score(np.argmax(y_test, axis=-1), predictions)\n",
        "\n",
        "# Displaying the computed metrics\n",
        "print('Accuracy:', accuracy.round(4))\n",
        "print('Precision:', precision.round(4))\n",
        "print('Recall:', recall.round(4))\n",
        "print('F1:', f1.round(4))\n",
        "\n",
        "# Plotting the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm.T, xticklabels=list(('healthy','unhealthy')), yticklabels=list(('healthy','unhealthy')), cmap='Blues', annot=True)\n",
        "plt.xlabel('True labels')\n",
        "plt.ylabel('Predicted labels')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "liW5p0zmguLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NgRnUaIPobk"
      },
      "source": [
        "## 3. Results and conclusions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtCRKkT3EMy0"
      },
      "outputs": [],
      "source": [
        "# Find the epoch with the highest validation accuracy\n",
        "best_epoch = np.argmax(history['val_accuracy'])\n",
        "\n",
        "# Plot training and validation performance metrics\n",
        "plt.figure(figsize=(20, 5))\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.plot(history['loss'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=3)\n",
        "plt.plot(history['val_loss'], label='Validation', alpha=0.8, color='#4D61E2', linewidth=3)\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Binary Crossentropy')\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.figure(figsize=(20, 5))\n",
        "\n",
        "# Plot training and validation accuracy, highlighting the best epoch\n",
        "plt.plot(history['accuracy'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=3)\n",
        "plt.plot(history['val_accuracy'], label='Validation', alpha=0.8, color='#4D61E2', linewidth=3)\n",
        "plt.plot(best_epoch, history['val_accuracy'][best_epoch], marker='*', alpha=0.8, markersize=10, color='#4D61E2')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Accuracy')\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# There is no overfitting -> good!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oszIBZ6WDK3V"
      },
      "outputs": [],
      "source": [
        "pred = model.predict(np.expand_dims(images[0], axis=0), verbose=0)\n",
        "pred, labels[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hHRTg9M_qAOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCbZG6izjZi-"
      },
      "source": [
        "## Train MobileNetV2 from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0N66p4OI2G8"
      },
      "outputs": [],
      "source": [
        "# Create MobileNetV2 model with specified settings\n",
        "mobile = tfk.applications.MobileNetV2(\n",
        "    input_shape=(96, 96, 3),\n",
        "    include_top=False,  # We avoid to include the fully connected layers that\n",
        "                        # are designed for a 1000-classes classification.\n",
        "    weights=None,       # We will not download the trained weights, we'll have\n",
        "                        # only the network (we train from skratch).\n",
        "    pooling='avg',      # Type of pooling.\n",
        ")\n",
        "\n",
        "# Display the model architecture with input shapes\n",
        "tfk.utils.plot_model(mobile, show_shapes=True)\n",
        "\n",
        "# DepthwiseConv2D is another different type of layer.\n",
        "\n",
        "# There are skip connections. The most important information are at the lowest\n",
        "# levels, in sequential networks I can extract information from the last layer,\n",
        "# adding skip connections, I'm able to exploit information form the both the\n",
        "# initial and final levels.\n",
        "\n",
        "# Using skip connections, I need to merge the information coming from different\n",
        "# layers in the correct way: in case of concatenation, I have to concatenate\n",
        "# kernels, in case of addition, I have need to have exactly the same shape of\n",
        "# the activations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pJtND4OjdOZ"
      },
      "outputs": [],
      "source": [
        "# Create an input layer with shape (96, 96, 3)\n",
        "inputs = tfk.Input(shape=(96, 96, 3))\n",
        "# Connect MobileNetV2 to the input\n",
        "x = mobile(inputs)\n",
        "# Add a Dense layer with 2 units and softmax activation as the classifier\n",
        "outputs = tfkl.Dense(2, activation='softmax')(x)\n",
        "\n",
        "# Create a Model connecting input and output\n",
        "model = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n",
        "\n",
        "# Compile the model with Categorical Cross-Entropy loss and Adam optimizer\n",
        "model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics=['accuracy'])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loZ7T818jdMB"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    x = preprocess_input(X_train*255), # We need to apply the preprocessing thought for the MobileNetV2 network\n",
        "    y = y_train,\n",
        "    batch_size = 16,\n",
        "    epochs = 200,\n",
        "    validation_data = (preprocess_input(X_val*255), y_val), # We need to apply the preprocessing thought for the MobileNetV2 network\n",
        "    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=20, restore_best_weights=True)]\n",
        ").history\n",
        "\n",
        "# Here we are training from skretch the network (this is not transfer learning).\n",
        "# We are overfitting a lot, the network is completely unable to generalise.\n",
        "# We can see that the accuracy on the training set is almost 100% and the\n",
        "# accuracy on the validation set is 50% (random guess).\n",
        "# The network is not able to generalise since training such a big network is not\n",
        "# easy and needs some steps to do and some particular techniques. Moreover here\n",
        "# we have a too small input dataset.\n",
        "\n",
        "# To train big networks we need to apply particular pipelines of learning rates,\n",
        "# we need to apply warmup (training using small learning rates)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2bVC89slWXK"
      },
      "outputs": [],
      "source": [
        "# Plot the re-trained MobileNetV2 training history\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(history['loss'], alpha=.3, color='#ff7f0e', linestyle='--')\n",
        "plt.plot(history['val_loss'], label='Re-trained', alpha=.8, color='#ff7f0e')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Categorical Crossentropy')\n",
        "plt.grid(alpha=.3)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(history['accuracy'], alpha=.3, color='#ff7f0e', linestyle='--')\n",
        "plt.plot(history['val_accuracy'], label='Re-trained', alpha=.8, color='#ff7f0e')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Accuracy')\n",
        "plt.grid(alpha=.3)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8S2rFtckdwU"
      },
      "source": [
        "## Transfer Learning\n",
        "Leveraging pre-trained network for a different classification problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEqu2E0c0moS"
      },
      "outputs": [],
      "source": [
        "mobile = tfk.applications.MobileNetV2(\n",
        "    input_shape=(96, 96, 3),\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    pooling='avg',\n",
        ")\n",
        "tfk.utils.plot_model(mobile, show_shapes=True)\n",
        "\n",
        "# Transfer learning: we want to freeze the parameters of the feature extraction\n",
        "# network and we want to train the classification head (fully connected NN)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VSxNoCw0mij"
      },
      "outputs": [],
      "source": [
        "# Use the supernet as feature extractor, i.e. freeze all its weigths\n",
        "mobile.trainable = False\n",
        "\n",
        "# Create an input layer with shape (224, 224, 3)\n",
        "inputs = tfk.Input(shape=(96, 96, 3))\n",
        "# Connect MobileNetV2 to the input\n",
        "x = mobile(inputs)\n",
        "# Add a Dense layer with 2 units and softmax activation as the classifier\n",
        "outputs = tfkl.Dense(2, activation='softmax')(x)\n",
        "\n",
        "# Create a Model connecting input and output\n",
        "tl_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n",
        "\n",
        "# Compile the model with Categorical Cross-Entropy loss and Adam optimizer\n",
        "tl_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics=['accuracy'])\n",
        "\n",
        "# Display model summary\n",
        "tl_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIsXmmkx0mf5"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "tl_history = tl_model.fit(\n",
        "    x = preprocess_input(X_train*255), # We need to apply the preprocessing thought for the MobileNetV2 network\n",
        "    y = y_train,\n",
        "    batch_size = 16,\n",
        "    epochs = 200,\n",
        "    validation_data = (preprocess_input(X_val*255), y_val), # We need to apply the preprocessing thought for the MobileNetV2 network\n",
        "    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=10, restore_best_weights=True)]\n",
        ").history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_Xk40zjIqu2"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test set\n",
        "test_accuracy = tl_model.evaluate(preprocess_input(X_test*255),y_test,verbose=0)[-1]\n",
        "print('Test set accuracy %.4f' % test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_lm0hLqlnlS"
      },
      "outputs": [],
      "source": [
        "# Plot the re-trained and the transfer learning MobileNetV2 training histories\n",
        "plt.figure(figsize=(15,5))\n",
        "#plt.plot(history['loss'], alpha=.3, color='#ff7f0e', linestyle='--')\n",
        "#plt.plot(history['val_loss'], label='Re-trained', alpha=.8, color='#ff7f0e')\n",
        "plt.plot(tl_history['loss'], alpha=.3, color='#4D61E2', linestyle='--')\n",
        "plt.plot(tl_history['val_loss'], label='Transfer Learning', alpha=.8, color='#4D61E2')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Categorical Crossentropy')\n",
        "plt.grid(alpha=.3)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "#plt.plot(history['accuracy'], alpha=.3, color='#ff7f0e', linestyle='--')\n",
        "#plt.plot(history['val_accuracy'], label='Re-trained', alpha=.8, color='#ff7f0e')\n",
        "plt.plot(tl_history['accuracy'], alpha=.3, color='#4D61E2', linestyle='--')\n",
        "plt.plot(tl_history['val_accuracy'], label='Transfer Learning', alpha=.8, color='#4D61E2')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Accuracy')\n",
        "plt.grid(alpha=.3)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# We achieved the performance of the network we designed in a small number of\n",
        "# epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXLw_oKSOkHd"
      },
      "outputs": [],
      "source": [
        "# Predict labels for the entire test set\n",
        "predictions = tl_model.predict(preprocess_input(X_test*255), verbose=0)\n",
        "\n",
        "# Display the shape of the predictions\n",
        "print(\"Predictions Shape:\", predictions.shape)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))\n",
        "\n",
        "# Compute classification metrics\n",
        "accuracy = accuracy_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))\n",
        "precision = precision_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
        "recall = recall_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
        "f1 = f1_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
        "\n",
        "# Display the computed metrics\n",
        "print('Accuracy:', accuracy.round(4))\n",
        "print('Precision:', precision.round(4))\n",
        "print('Recall:', recall.round(4))\n",
        "print('F1:', f1.round(4))\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm.T, xticklabels=list(('healthy','unhealthy')), yticklabels=list(('healthy','unhealthy')), cmap='Blues', annot=True)\n",
        "plt.xlabel('True labels')\n",
        "plt.ylabel('Predicted labels')\n",
        "plt.show()\n",
        "\n",
        "# Still good performances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApkXA-EV0mcz"
      },
      "outputs": [],
      "source": [
        "# Save the best model\n",
        "tl_model.save('TransferLearningModel_2')\n",
        "#del tl_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fYvl9QtST4T"
      },
      "outputs": [],
      "source": [
        "m = tf.keras.models.load_model('trial')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EziuvSdmRWU0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
        "\n",
        "class model:\n",
        "    def __init__(self, path):\n",
        "        self.model = tf.keras.models.load_model( 'trial')\n",
        "\n",
        "    def predict(self, X):\n",
        "\n",
        "        # Note: this is just an example.\n",
        "        # Here the model.predict is called, followed by the argmax\n",
        "        out = self.model.predict(preprocess_input(X*255))\n",
        "        out = tf.argmax(out, axis=-1)  # Shape [BS]\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-CUwOK5RbtA"
      },
      "outputs": [],
      "source": [
        "model = model(os.getcwd())\n",
        "pred = model.predict(X_test)\n",
        "pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzy3kPeIS-Zj"
      },
      "outputs": [],
      "source": [
        "np.argmax(predictions, axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tV2kfM9NlLJh"
      },
      "source": [
        "## Fine Tuning\n",
        "Training also the backbone network. To this purpose it is better to\n",
        "1.   Perform transfer learning, namely train the new classification head only;\n",
        "2.   Unfreeze the backbone layers (or possibly a few of them depending on the amount of training data) and train the entire netrwork. Note that the classification head needs to be a properly trained model. We adjust the last set of transformations, we use a very small learning rate.\n",
        "\n",
        "Training a neural network cannot be done end-to-end. Most effective trainings are split in many steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlL1Yb_GIIbr"
      },
      "outputs": [],
      "source": [
        "# Re-load the model after transfer learning\n",
        "ft_model = tfk.models.load_model('TransferLearningModel_1')\n",
        "ft_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q68cKl2W0mXT"
      },
      "outputs": [],
      "source": [
        "# Set all MobileNetV2 layers as trainable\n",
        "ft_model.get_layer('mobilenetv2_1.00_96').trainable = True\n",
        "for i, layer in enumerate(ft_model.get_layer('mobilenetv2_1.00_96').layers):\n",
        "   print(i, layer.name, layer.trainable)\n",
        "\n",
        "# I can also iteratively unfreeze only some parts of the network and do the\n",
        "# training, e.g. the first time, only the last layer; the second time, only the\n",
        "# last two, and so on.\n",
        "# We can also unfreeze different parts of the networks in different steps of the\n",
        "# training keeping freezed the others, training alternatively different parts of\n",
        "# the network. We can change the learning rates training different parts (e.g.\n",
        "# higher learning rates on the last layers and smaller learning rates in the\n",
        "# case of the training of the CNN layers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrasXwPsF1zi"
      },
      "outputs": [],
      "source": [
        "# Freeze first N layers, e.g., until the 133rd one\n",
        "N = 133\n",
        "for i, layer in enumerate(ft_model.get_layer('mobilenetv2_1.00_96').layers[:N]):\n",
        "  layer.trainable=False\n",
        "for i, layer in enumerate(ft_model.get_layer('mobilenetv2_1.00_96').layers):\n",
        "   print(i, layer.name, layer.trainable)\n",
        "ft_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWdPyqwRF1wk"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "ft_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(1e-5), metrics='accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XveYFnj2F1tz"
      },
      "outputs": [],
      "source": [
        "# Fine-tune the model\n",
        "ft_history = ft_model.fit(\n",
        "    x = preprocess_input(X_train*255), # We need to apply the preprocessing thought for the MobileNetV2 network\n",
        "    y = y_train,\n",
        "    batch_size = 16,\n",
        "    epochs = 200,\n",
        "    validation_data = (preprocess_input(X_val*255), y_val), # We need to apply the preprocessing thought for the MobileNetV2 network\n",
        "    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=20, restore_best_weights=True)]\n",
        ").history\n",
        "\n",
        "# The first epoch is already more performing than all the previous model we have\n",
        "# created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ji6ON_n0F1qs"
      },
      "outputs": [],
      "source": [
        "# Plot the re-trained, the transfer learning and the fine-tuned MobileNetV2 training histories\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(history['loss'], alpha=.3, color='#ff7f0e', linestyle='--')\n",
        "plt.plot(history['val_loss'], label='Re-trained', alpha=.8, color='#ff7f0e')\n",
        "plt.plot(tl_history['loss'], alpha=.3, color='#4D61E2', linestyle='--')\n",
        "plt.plot(tl_history['val_loss'], label='Transfer Learning', alpha=.8, color='#4D61E2')\n",
        "plt.plot(ft_history['loss'], alpha=.3, color='#408537', linestyle='--')\n",
        "plt.plot(ft_history['val_loss'], label='Fine Tuning', alpha=.8, color='#408537')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Binary Crossentropy')\n",
        "plt.grid(alpha=.3)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(history['accuracy'], alpha=.3, color='#ff7f0e', linestyle='--')\n",
        "plt.plot(history['val_accuracy'], label='Re-trained', alpha=.8, color='#ff7f0e')\n",
        "plt.plot(tl_history['accuracy'], alpha=.3, color='#4D61E2', linestyle='--')\n",
        "plt.plot(tl_history['val_accuracy'], label='Transfer Learning', alpha=.8, color='#4D61E2')\n",
        "plt.plot(ft_history['accuracy'], alpha=.3, color='#408537', linestyle='--')\n",
        "plt.plot(ft_history['val_accuracy'], label='Fine Tuning', alpha=.8, color='#408537')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Accuracy')\n",
        "plt.grid(alpha=.3)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# We can also skip the transfer learning step but it will be more difficult for\n",
        "# the network to learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "df2h3LUsCAcp"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test set\n",
        "test_accuracy = ft_model.evaluate(preprocess_input(X_test*255),y_test,verbose=0)[-1]\n",
        "print('Test set accuracy %.4f' % test_accuracy)\n",
        "\n",
        "# Very good performance on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igVxKHv2Lp1R"
      },
      "outputs": [],
      "source": [
        "# Predict labels for the entire test set\n",
        "predictions = ft_model.predict(preprocess_input(X_test*255), verbose=0)\n",
        "\n",
        "# Display the shape of the predictions\n",
        "print(\"Predictions Shape:\", predictions.shape)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))\n",
        "\n",
        "# Compute classification metrics\n",
        "accuracy = accuracy_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))\n",
        "precision = precision_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
        "recall = recall_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
        "f1 = f1_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
        "\n",
        "# Display the computed metrics\n",
        "print('Accuracy:', accuracy.round(4))\n",
        "print('Precision:', precision.round(4))\n",
        "print('Recall:', recall.round(4))\n",
        "print('F1:', f1.round(4))\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm.T, xticklabels=list(('healthy','unhealthy')), yticklabels=list(('healthy','unhealthy')), cmap='Blues', annot=True)\n",
        "plt.xlabel('True labels')\n",
        "plt.ylabel('Predicted labels')\n",
        "plt.show()\n",
        "\n",
        "# Still good performances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ih9zLQIenGSF"
      },
      "outputs": [],
      "source": [
        "# Save the best model\n",
        "ft_model.save('FineTuningModel')\n",
        "del ft_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tigJr33nE43"
      },
      "source": [
        "### Test on your own data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCCDJMttF1nv"
      },
      "outputs": [],
      "source": [
        "# Re-load the model after transfer learning\n",
        "ft_model = tfk.models.load_model('FineTuningModel')\n",
        "ft_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGp7v_zHnWgO"
      },
      "outputs": [],
      "source": [
        "test_path = 'test_images/'\n",
        "test = load_images_from_folder(test_path)\n",
        "test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sx1PCVznhsW"
      },
      "outputs": [],
      "source": [
        "# predict the test image\n",
        "test_predictions = ft_model.predict(preprocess_input(test*255), verbose=0)\n",
        "\n",
        "index = 2\n",
        "prediction = test_predictions[index]\n",
        "\n",
        "if prediction[0] >= prediction[1]:\n",
        "    label = 'item'\n",
        "    probability = float(\"{:.{}f}\".format(prediction[0], 4))*100\n",
        "\n",
        "else:\n",
        "    label = 'animal'\n",
        "    probability = float(\"{:.{}f}\".format(prediction[1], 4))*100\n",
        "\n",
        "plt.title(f'{label} ({probability}%)')\n",
        "plt.imshow(np.squeeze(test[index]))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVMf_4y9z2tI"
      },
      "source": [
        "### TODO Exercise\n",
        "Practice the various transformations, create your own successful preprocessing pipeline and compare different Keras applications' architectures.\n",
        "\n",
        "What happens if you combine augmentation techniques with fine tuning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4tWF6oUQFqH"
      },
      "source": [
        "*Credits: Eugenio Lomurno, 📧 eugenio.lomurno@polimi.it*"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "53DAEfQuI41_",
        "jeEEWbr_Gpod",
        "Pfe2Rg9xKXlt",
        "AwBMvcHXFCEX",
        "4EEFEf9IMI-3",
        "Lja4vs4jXZU8",
        "5tyiI1izkUyW",
        "UfTeR7dsSVPc",
        "EfWlnSNCSVlH",
        "a3bNxziaN1Mv",
        "d1pkUl5POslo",
        "3SLXNVuQUXak",
        "PlRnp4WOPd_J",
        "8Y6QkLb77UD7",
        "JXNbiD6koUFE",
        "1NgRnUaIPobk",
        "cCbZG6izjZi-",
        "T8S2rFtckdwU",
        "tV2kfM9NlLJh"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}